{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f80de9c-b558-4f77-8df4-29c273a892ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14014/3247753816.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"numpy.float64\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 352\u001b[0m\n\u001b[1;32m    350\u001b[0m         result\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39mabs(h(t[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i, \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mwhere(result\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(result))\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"numpy.float64\") to str"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sympy\n",
    "from sympy import Symbol, Mul, Pow, Add\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Prints the current allocated, reserved, and free memory on the GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "        free_memory = torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device)  # Free memory in reserved area\n",
    "        free_memory_mb = free_memory / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "        print(f\"Allocated memory: {allocated_memory:.2f} MB\")\n",
    "        print(f\"Reserved memory: {reserved_memory:.2f} MB\")\n",
    "        print(f\"Free memory: {free_memory_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "t=sympy.Symbol('t')\n",
    "sqrt_3=torch.sqrt(torch.tensor([3.])).cuda()\n",
    "\n",
    "\n",
    "def complex_cbrt(z):\n",
    "    \"\"\"Calculate the cubic root of a complex number.\"\"\"\n",
    "    # Calculate the magnitude and angle of the complex number\n",
    "    r = z.abs()  # Magnitude\n",
    "    theta = torch.angle(z)  # Angle in radians\n",
    "    #print(\"z\")\n",
    "    #print(z)\n",
    "    #print(\"r\")\n",
    "    #print(r)\n",
    "    #print(\"theta\")\n",
    "    #print(theta)\n",
    "\n",
    "    # Calculate the cubic root of the magnitude and the angle divided by 3\n",
    "    r = r ** (1/3)  # Magnitude of the cubic root\n",
    "    theta = theta / 3  # Angle of the cubic root\n",
    "    #print(\"root_magnitude\")\n",
    "    #print(r)\n",
    "    #print(\"root_angle\")\n",
    "    #print(theta)\n",
    "\n",
    "    return r * torch.cos(theta) + 1j * r * torch.sin(theta)  # Return as a complex number\n",
    "\n",
    "def quartic_solver(a,b,c,d,e):\n",
    "    e = e/a\n",
    "    d = d/a\n",
    "    c = c/a\n",
    "    b = b/a\n",
    "    i, lr_1 = torch.frexp(e)\n",
    "    i, lr_2 = torch.frexp(d)\n",
    "    i, lr_3 = torch.frexp(c)\n",
    "    i, lr_4 = torch.frexp(b)\n",
    "    i = torch.max(\n",
    "        torch.cat((2 + 3 * torch.max(torch.cat((2 * lr_3, 2 + lr_4 + lr_2, 4 + lr_1), \n",
    "                                           dim = -1), dim = -1, keepdim = True)[0], \n",
    "                  2 * torch.max(torch.cat((1 + 3 * lr_3, 3 + lr_4 + lr_3 + lr_2, 5 + 2 * lr_4 + lr_1, 5 + 2 * lr_2, 8 + lr_3 + lr_1), \n",
    "                                        dim = -1), dim = -1, keepdim = True)[0]), \n",
    "                  dim = -1), dim = -1, keepdim = True)[0]//12\n",
    "        \n",
    "    a = c ** 2 - 3 * b * d + 12 * e\n",
    "    e = 2 * c ** 3 - 9 * b * c * d + 27 * b ** 2 * e + 27 * d ** 2 - 72 * c * e\n",
    "    e = torch.ldexp(e, -6*i)\n",
    "    a = torch.ldexp(a, -4*i)\n",
    "    #print(\"Delta_0\")\n",
    "    #print(a)\n",
    "    #print(\"Delta_1\")\n",
    "    #print(e)\n",
    "    #print(\"under sqrt\")\n",
    "    #print(e ** 2 - 4 * a ** 3)\n",
    "    #print(\"sqrt\")\n",
    "    #print(torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64)))\n",
    "\n",
    "    Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2) \n",
    "    #print(\"Q\")\n",
    "    #print(Q)\n",
    "    \n",
    "    # Calculate p and q\n",
    "    e = torch.ldexp(c - 0.375 * b ** 2, -2*i)\n",
    "    d = torch.ldexp((0.5*b) ** 3 - 0.5 * b * c + d, -3*i)\n",
    "    #print(\"q\")\n",
    "    #print(d)\n",
    "    S = torch.sqrt(-2 / 3 * e + (Q + a / Q) / 3) / 2\n",
    "    a = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e + d / S)\n",
    "    c = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e - d / S)\n",
    "    b = torch.ldexp(-b, -i-2)\n",
    "\n",
    "    lr_1 = torch.ldexp(torch.real(b - S + a), i)\n",
    "    lr_2 = torch.ldexp(torch.real(b - S - a), i)\n",
    "    lr_3 = torch.ldexp(torch.real(b + S + c), i)\n",
    "    lr_4 = torch.ldexp(torch.real(b + S - c), i)\n",
    "    \n",
    "    return torch.cat([lr_1, lr_2, lr_3, lr_4],dim = -1)\n",
    "\n",
    "def cubic_solver(a,b,c,d):\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    a = c - b ** 2 / 3\n",
    "    d = 2/27*b**3 - b*c/3 + d\n",
    "    c = (a/3)**3 + (d/2)**2\n",
    "    c = torch.sqrt(c)\n",
    "    alpha = -d/2 + c\n",
    "    beta = -d/2 - c\n",
    "    alpha = complex_cbrt(alpha)\n",
    "    beta = complex_cbrt(beta)\n",
    "    \n",
    "    lr1 = torch.real(alpha+beta)\n",
    "    lr2 = torch.real((-(alpha+beta) + 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    lr3 = torch.real((-(alpha+beta) - 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    return torch.cat([lr1, lr2, lr3, torch.zeros_like(lr1)],dim=-1)\n",
    "\n",
    "\n",
    "def quadratic_solver(c,d,e):\n",
    "    D = torch.sqrt(d ** 2 - 4 * e * c)\n",
    "    x1 = torch.real((-d + D)/(2*c))\n",
    "    x2 = torch.real((-d - D)/(2*c))\n",
    "    return torch.cat([x1, x2, torch.zeros_like(x1), torch.zeros_like(x1)], dim=-1)\n",
    "\n",
    "def polynomial_solver(a,b,c,d,e):\n",
    "    return torch.where(a!=0, \n",
    "                       quartic_solver(a,b,c,d,e), torch.where(\n",
    "                        b!=0,\n",
    "                        cubic_solver(b,c,d,e), torch.where(\n",
    "                        c!=0,\n",
    "                        quadratic_solver(c,d,e), torch.where(\n",
    "                        d!=0, \n",
    "                        (e/d).repeat((len(a.shape)-1)*(1,)+(4,)), \n",
    "                        e.repeat((len(a.shape)-1)*(1,)+(4,))))))\n",
    "\n",
    "\n",
    "def optimal_lr(A, x):\n",
    "    \"\"\"Calculate optimal learning rate based on matrix A and vector x.\"\"\"\n",
    "    # Compute intermediate vectors\n",
    "    u = A(x)\n",
    "    v = A(u)\n",
    "    w = A(v)\n",
    "    \n",
    "    # Compute inner products\n",
    "    a_0 = torch.sum(x*x, dim = -1, keepdim = True)\n",
    "    a_1 = torch.sum(u*x, dim = -1, keepdim = True)\n",
    "    a_2 = torch.sum(v*x, dim = -1, keepdim = True)\n",
    "    a_3 = torch.sum(w*x, dim = -1, keepdim = True)\n",
    "    a_4 = torch.sum(w*u, dim = -1, keepdim = True)\n",
    "    a_5 = torch.sum(w*v, dim = -1, keepdim = True)\n",
    "    a_6 = torch.sum(w*w, dim = -1, keepdim = True)\n",
    "\n",
    "    # Calculate r_0, r_1, r_2\n",
    "    r_0 = 4 * a_2 / (a_1 * a_1) - 2 * a_1 / (a_1 * a_0) - 2 * a_3 / (a_1 * a_2) - 2 * a_3 / (a_1 * a_2) + a_4 / (a_2 * a_2) + a_2 / (a_0 * a_2) - 2 * a_1 / (a_1 * a_0) + a_0 / (a_0 * a_0) + a_2 / (a_0 * a_2)\n",
    "    r_1 = 4 * a_3 / (a_1 * a_1) - 2 * a_2 / (a_1 * a_0) - 2 * a_4 / (a_1 * a_2) - 2 * a_4 / (a_1 * a_2) + a_5 / (a_2 * a_2) + a_3 / (a_0 * a_2) - 2 * a_2 / (a_1 * a_0) + a_1 / (a_0 * a_0) + a_3 / (a_0 * a_2)\n",
    "    r_2 = 4 * a_4 / (a_1 * a_1) - 2 * a_3 / (a_1 * a_0) - 2 * a_5 / (a_1 * a_2) - 2 * a_5 / (a_1 * a_2) + a_6 / (a_2 * a_2) + a_4 / (a_0 * a_2) - 2 * a_3 / (a_1 * a_0) + a_2 / (a_0 * a_0) + a_4 / (a_0 * a_2)\n",
    "\n",
    "    # Calculate q and p\n",
    "    q_1 = 2 * a_2 / a_1 - a_1 / a_0 - a_3 / a_2\n",
    "    q_2 = 2 * a_3 / a_1 - a_2 / a_0 - a_4 / a_2\n",
    "    #p_0 = a_0\n",
    "    #p_1 = a_1\n",
    "    #p_2 = a_2\n",
    "\n",
    "    a = r_0 * r_1 * q_2 - 2 * r_0 * q_1 * r_2\n",
    "    b = a_0 * r_1 * r_2 - 2 * a_1 * r_0 * r_2 + a_2 * r_0 * r_1 - 2 * q_1 * q_2 * r_0\n",
    "    c = 3 * a_0 * r_1 * q_2 - 3 * r_0 * a_1 * q_2\n",
    "    d = 2 * a_0 * q_1 * q_2 + 2 * a_0 * r_1 * a_2 - a_0 * a_1 * r_2 - r_0 * a_1 * a_2\n",
    "    e = 2 * a_0 * q_1 * a_2 - a_0 * a_1 * q_2\n",
    "    lr = polynomial_solver(a,b,c,d,e)\n",
    "    g = a_1 + 2 * lr * q_1 + lr ** 2 * r_1\n",
    "    f = a_0 + lr ** 2 * r_0\n",
    "    h = a_2 + 2 * lr * q_2 + lr ** 2 * r_2\n",
    "    eigenness = g**2/(f*h)\n",
    "    n = torch.argmax(eigenness, dim=-1, keepdim=True)\n",
    "    lr = torch.gather(lr, -1, n)\n",
    "    return torch.gather(eigenness, -1, n), lr * (-x / a_0 + 2 * u / a_1 - v / a_2), n, lr\n",
    "    return torch.gather(eigenness, -1, n), (-lr / a_0) * x + (2 * lr / a_1) * u + (-lr / a_2) * v, n, lr\n",
    "\n",
    "\n",
    "def update_vector(x, v):\n",
    "    x = x + v\n",
    "    return x/torch.linalg.norm(x, dim = -1, keepdim = True)\n",
    "\n",
    "def grad_ascend_lr(A,x,threshold,steps_already,steps_max):\n",
    "    e,v,i,l = optimal_lr(A, x)\n",
    "    x = update_vector(x, v)\n",
    "    steps = steps_already + 1\n",
    "    cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    while cond.any():\n",
    "        steps = torch.where(cond, steps + 1, steps)\n",
    "        e,v,i,l = optimal_lr(A, x)\n",
    "        x = torch.where(cond, update_vector(x, v), x)\n",
    "        cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    v=[]\n",
    "    f=A(x)\n",
    "    #print_gpu_memory()\n",
    "    return x, torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1).unsqueeze(-1)**2, (\n",
    "            f-(torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1)).unsqueeze(-1)**2*x\n",
    "            )/torch.linalg.norm(f, dim=-1).unsqueeze(-1), steps\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "class square(nn.Module):\n",
    "    def __init__(self, g):\n",
    "        super(square, self).__init__()\n",
    "        self.g=g\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.g(self.g(x))\n",
    "\n",
    "class minus_f(nn.Module):\n",
    "    def __init__(self, g, f):\n",
    "        super(minus_f, self).__init__()\n",
    "        self.g=g\n",
    "        self.f=f\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.g(x) - self.f*x\n",
    "\n",
    "class t_minus(nn.Module):\n",
    "    def __init__(self, g, t):\n",
    "        super(t_minus, self).__init__()\n",
    "        self.g=g\n",
    "        self.t=t\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.t*x - self.g(x)\n",
    "\n",
    "\n",
    "class modified(nn.Module):\n",
    "    def __init__(self,expr,A,batch_size):\n",
    "        super(modified, self).__init__()\n",
    "        self.parts=[]\n",
    "        for arg in expr.args:\n",
    "            self.parts.append(modified(arg,A,batch_size))\n",
    "        if expr.is_Number:\n",
    "            self.param = nn.Parameter(torch.random.uniform((batch_size,1))*float(expr))\n",
    "        self.f=torch.rand(batch_size,1).cuda()\n",
    "        self.t=((torch.rand(batch_size,1).cuda())*torch.max(torch.cat([self.f**2, (1-self.f)**2], dim=1), dim=1, keepdim = True)[0]).cuda()\n",
    "        self.z=torch.max(torch.cat([self.t**2, (self.t-self.f**2)**2, (self.t-(1-self.f)**2)**2], dim=1), dim=1, keepdim = True)[0]\n",
    "        if expr == sympy.Symbol('t'):\n",
    "            self.A=A\n",
    "        self.expr=expr\n",
    "        self.minus_f = minus_f(self.A, self.f)\n",
    "        self.square_1 = square(self.minus_f)\n",
    "        self.t_minus = t_minus(self.square_1, self.t)\n",
    "        self.square_2 = square(self.t_minus)\n",
    "    \n",
    "    def forward_1(self,x):\n",
    "        return x - self.square_2(x)/self.z\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.forward_1(x)\n",
    "        y = self.forward_1(y)\n",
    "        y = self.forward_1(y)\n",
    "        return self.forward_1(y) \n",
    "        if self.expr == sympy.Symbol('t'):\n",
    "            return self.A(x)\n",
    "\n",
    "        if self.expr.is_Number:\n",
    "            return self.param*x\n",
    "\n",
    "        if self.expr.is_Add:\n",
    "            # Initialize res as float zeros to avoid dtype conflicts\n",
    "            res = torch.zeros_like(x)\n",
    "            for arg in self.parts:\n",
    "                res += arg(x)\n",
    "            return res\n",
    "\n",
    "        if self.expr.is_Mul:\n",
    "            # product: to mimic polynomial multiplication\n",
    "            for arg in self.parts:\n",
    "                x = arg(x)\n",
    "            return x\n",
    "\n",
    "        if self.expr.is_Pow:\n",
    "            for _ in range(self.parts[1]):\n",
    "                x = self.parts[0](x)\n",
    "            return x\n",
    " \n",
    "\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "batch_size=16384\n",
    "matrix_size=256\n",
    "A=torch.diag(torch.tensor(range(matrix_size), dtype=torch.float64)/matrix_size)\n",
    "\n",
    "import scipy\n",
    "\n",
    "C = torch.tensor(scipy.stats.ortho_group.rvs(dim=matrix_size), dtype=torch.float64)\n",
    "A = torch.linalg.inv(C) @ A @ C\n",
    "\n",
    "f=nn.Linear(matrix_size,matrix_size, bias=False).cuda()\n",
    "f.weight=nn.Parameter(A.cuda(), requires_grad=False)\n",
    "#f.bias=nn.Parameter(torch.zeros_like(f.bias))\n",
    "\n",
    "h = nn.Linear(matrix_size, matrix_size, bias=False).cuda()\n",
    "h.weight=nn.Parameter(C.cuda(), requires_grad=False)\n",
    "\n",
    "probs = 0.5*torch.ones((batch_size, 1)).cuda()\n",
    "delta = torch.tensor(1e-6).cuda()\n",
    "\n",
    "class truncated(nn.Module):\n",
    "    def __init__(self, f, x):\n",
    "        super(truncated, self).__init__()\n",
    "        \n",
    "        y = f(x)\n",
    "        xx = torch.sum(x**2, dim = -1, keepdim = True)\n",
    "        xAx = torch.sum(x*y, dim = -1, keepdim = True)\n",
    "        xAAx = torch.sum(y**2, dim = -1, keepdim = True)\n",
    "        mean = xAx/xx\n",
    "        sigma = torch.sqrt(xAAx*xx - xAx**2)/xx\n",
    "        self.mean = mean + (-1)**torch.bernoulli(probs) * sigma\n",
    "        self.sigma = torch.sqrt(-2*sigma*torch.log(delta))\n",
    "        self.lambda_min = self.mean - self.sigma\n",
    "        self.lambda_max = self.mean + self.sigma\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.f(x) - self.mean * x\n",
    "        y = self.f(y) - self.mean * y\n",
    "        return x - y / torch.min(torch.cat((\n",
    "            (self.lambda_min - self.mean)**2, \n",
    "            (self.lambda_max - self.mean)**2), \n",
    "                dim = -1), dim = -1, keepdim = True)[0]\n",
    "\n",
    "result=np.zeros((matrix_size,))\n",
    "expr_1=t\n",
    "print(expr_1)\n",
    "t=[1]\n",
    "stepses=[]\n",
    "with torch.cuda.device(0):\n",
    "    for i in range(1):\n",
    "        x = torch.empty((batch_size,matrix_size), dtype = torch.float64).normal_(mean=0,std=1).cuda()\n",
    "        steps = torch.zeros((batch_size,1)).cuda()\n",
    "        t[0] = x/torch.linalg.norm(x, dim=-1, keepdim=True)\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim=-1, keepdim=True)[0]<0.9)\n",
    "        while torch.any((steps<1000) & cond):\n",
    "            g = modified(expr_1,f,batch_size)\n",
    "            s = grad_ascend_lr(g,t[0],1e-6,steps,1000)\n",
    "            t[0] = torch.where(cond, s[0], t[0])\n",
    "            steps = torch.where(cond, s[-1], steps)\n",
    "            s = [1]\n",
    "            cond = (torch.max(torch.abs(h(t[0])), dim=-1, keepdim=True)[0]<0.9)\n",
    "            print(i, list(np.where(torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=0).cpu().numpy()>0)[0]), end='\\r')\n",
    "        result+= torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=0).cpu().numpy()\n",
    "        print(i, list(np.where(result>0)[0]), end='\\r')\n",
    "print('\\n'+np.sum(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba760433-5f08-4292-a236-b73a50a14d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4647.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23d3f74f-972a-4aea-af77-cf8f6a710602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.where(result>0, 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "521d1a26-13fd-42ba-8dd4-fa6f58a19e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([160, 172]),)\n"
     ]
    }
   ],
   "source": [
    "print(np.where(np.where(result>0, 0, 1)>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "227179c1-6f58-4c7b-a799-e3d50eec26d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(steps.cpu().numpy()[np.where(torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=1,keepdim=True).cpu().numpy()>0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "167c5b1d-5c6a-4715-a18f-c4ae2f304527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 506.,  624.,  779.,  781.,  783.,  826.,  866.,  880.,  887.,\n",
       "        922.,  947.,  961.,  965.,  966.,  981.,  983.,  989.,  990.,\n",
       "        999., 1000.], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(steps.cpu().numpy()[np.where(torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=1,keepdim=True).cpu().numpy()>0)])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bb6f42f-31ec-4463-aec9-b996b85e74b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14014/2464278850.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 28, 29, 30, 32, 34, 35, 37, 38, 39, 41, 44, 45, 49, 60]\r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"numpy.float64\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 352\u001b[0m\n\u001b[1;32m    350\u001b[0m         result\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39mabs(h(t[\u001b[38;5;241m0\u001b[39m]))\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i, \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mwhere(result\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(result))\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"numpy.float64\") to str"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sympy\n",
    "from sympy import Symbol, Mul, Pow, Add\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Prints the current allocated, reserved, and free memory on the GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "        free_memory = torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device)  # Free memory in reserved area\n",
    "        free_memory_mb = free_memory / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "        print(f\"Allocated memory: {allocated_memory:.2f} MB\")\n",
    "        print(f\"Reserved memory: {reserved_memory:.2f} MB\")\n",
    "        print(f\"Free memory: {free_memory_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "t=sympy.Symbol('t')\n",
    "sqrt_3=torch.sqrt(torch.tensor([3.])).cuda()\n",
    "\n",
    "\n",
    "def complex_cbrt(z):\n",
    "    \"\"\"Calculate the cubic root of a complex number.\"\"\"\n",
    "    # Calculate the magnitude and angle of the complex number\n",
    "    r = z.abs()  # Magnitude\n",
    "    theta = torch.angle(z)  # Angle in radians\n",
    "    #print(\"z\")\n",
    "    #print(z)\n",
    "    #print(\"r\")\n",
    "    #print(r)\n",
    "    #print(\"theta\")\n",
    "    #print(theta)\n",
    "\n",
    "    # Calculate the cubic root of the magnitude and the angle divided by 3\n",
    "    r = r ** (1/3)  # Magnitude of the cubic root\n",
    "    theta = theta / 3  # Angle of the cubic root\n",
    "    #print(\"root_magnitude\")\n",
    "    #print(r)\n",
    "    #print(\"root_angle\")\n",
    "    #print(theta)\n",
    "\n",
    "    return r * torch.cos(theta) + 1j * r * torch.sin(theta)  # Return as a complex number\n",
    "\n",
    "def quartic_solver(a,b,c,d,e):\n",
    "    e = e/a\n",
    "    d = d/a\n",
    "    c = c/a\n",
    "    b = b/a\n",
    "    i, lr_1 = torch.frexp(e)\n",
    "    i, lr_2 = torch.frexp(d)\n",
    "    i, lr_3 = torch.frexp(c)\n",
    "    i, lr_4 = torch.frexp(b)\n",
    "    i = torch.max(\n",
    "        torch.cat((2 + 3 * torch.max(torch.cat((2 * lr_3, 2 + lr_4 + lr_2, 4 + lr_1), \n",
    "                                           dim = -1), dim = -1, keepdim = True)[0], \n",
    "                  2 * torch.max(torch.cat((1 + 3 * lr_3, 3 + lr_4 + lr_3 + lr_2, 5 + 2 * lr_4 + lr_1, 5 + 2 * lr_2, 8 + lr_3 + lr_1), \n",
    "                                        dim = -1), dim = -1, keepdim = True)[0]), \n",
    "                  dim = -1), dim = -1, keepdim = True)[0]//12\n",
    "        \n",
    "    a = c ** 2 - 3 * b * d + 12 * e\n",
    "    e = 2 * c ** 3 - 9 * b * c * d + 27 * b ** 2 * e + 27 * d ** 2 - 72 * c * e\n",
    "    e = torch.ldexp(e, -6*i)\n",
    "    a = torch.ldexp(a, -4*i)\n",
    "    #print(\"Delta_0\")\n",
    "    #print(a)\n",
    "    #print(\"Delta_1\")\n",
    "    #print(e)\n",
    "    #print(\"under sqrt\")\n",
    "    #print(e ** 2 - 4 * a ** 3)\n",
    "    #print(\"sqrt\")\n",
    "    #print(torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64)))\n",
    "\n",
    "    Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2) \n",
    "    #print(\"Q\")\n",
    "    #print(Q)\n",
    "    \n",
    "    # Calculate p and q\n",
    "    e = torch.ldexp(c - 0.375 * b ** 2, -2*i)\n",
    "    d = torch.ldexp((0.5*b) ** 3 - 0.5 * b * c + d, -3*i)\n",
    "    #print(\"q\")\n",
    "    #print(d)\n",
    "    S = torch.sqrt(-2 / 3 * e + (Q + a / Q) / 3) / 2\n",
    "    a = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e + d / S)\n",
    "    c = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e - d / S)\n",
    "    b = torch.ldexp(-b, -i-2)\n",
    "\n",
    "    lr_1 = torch.ldexp(torch.real(b - S + a), i)\n",
    "    lr_2 = torch.ldexp(torch.real(b - S - a), i)\n",
    "    lr_3 = torch.ldexp(torch.real(b + S + c), i)\n",
    "    lr_4 = torch.ldexp(torch.real(b + S - c), i)\n",
    "    \n",
    "    return torch.cat([lr_1, lr_2, lr_3, lr_4],dim = -1)\n",
    "\n",
    "def cubic_solver(a,b,c,d):\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    a = c - b ** 2 / 3\n",
    "    d = 2/27*b**3 - b*c/3 + d\n",
    "    c = (a/3)**3 + (d/2)**2\n",
    "    c = torch.sqrt(c)\n",
    "    alpha = -d/2 + c\n",
    "    beta = -d/2 - c\n",
    "    alpha = complex_cbrt(alpha)\n",
    "    beta = complex_cbrt(beta)\n",
    "    \n",
    "    lr1 = torch.real(alpha+beta)\n",
    "    lr2 = torch.real((-(alpha+beta) + 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    lr3 = torch.real((-(alpha+beta) - 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    return torch.cat([lr1, lr2, lr3, torch.zeros_like(lr1)],dim=-1)\n",
    "\n",
    "\n",
    "def quadratic_solver(c,d,e):\n",
    "    D = torch.sqrt(d ** 2 - 4 * e * c)\n",
    "    x1 = torch.real((-d + D)/(2*c))\n",
    "    x2 = torch.real((-d - D)/(2*c))\n",
    "    return torch.cat([x1, x2, torch.zeros_like(x1), torch.zeros_like(x1)], dim=-1)\n",
    "\n",
    "def polynomial_solver(a,b,c,d,e):\n",
    "    return torch.where(a!=0, \n",
    "                       quartic_solver(a,b,c,d,e), torch.where(\n",
    "                        b!=0,\n",
    "                        cubic_solver(b,c,d,e), torch.where(\n",
    "                        c!=0,\n",
    "                        quadratic_solver(c,d,e), torch.where(\n",
    "                        d!=0, \n",
    "                        (e/d).repeat((len(a.shape)-1)*(1,)+(4,)), \n",
    "                        e.repeat((len(a.shape)-1)*(1,)+(4,))))))\n",
    "\n",
    "\n",
    "def optimal_lr(A, x):\n",
    "    \"\"\"Calculate optimal learning rate based on matrix A and vector x.\"\"\"\n",
    "    # Compute intermediate vectors\n",
    "    u = A(x)\n",
    "    v = A(u)\n",
    "    w = A(v)\n",
    "    \n",
    "    # Compute inner products\n",
    "    a_0 = torch.sum(x*x, dim = -1, keepdim = True)\n",
    "    a_1 = torch.sum(u*x, dim = -1, keepdim = True)\n",
    "    a_2 = torch.sum(v*x, dim = -1, keepdim = True)\n",
    "    a_3 = torch.sum(w*x, dim = -1, keepdim = True)\n",
    "    a_4 = torch.sum(w*u, dim = -1, keepdim = True)\n",
    "    a_5 = torch.sum(w*v, dim = -1, keepdim = True)\n",
    "    a_6 = torch.sum(w*w, dim = -1, keepdim = True)\n",
    "\n",
    "    # Calculate r_0, r_1, r_2\n",
    "    r_0 = 4 * a_2 / (a_1 * a_1) - 2 * a_1 / (a_1 * a_0) - 2 * a_3 / (a_1 * a_2) - 2 * a_3 / (a_1 * a_2) + a_4 / (a_2 * a_2) + a_2 / (a_0 * a_2) - 2 * a_1 / (a_1 * a_0) + a_0 / (a_0 * a_0) + a_2 / (a_0 * a_2)\n",
    "    r_1 = 4 * a_3 / (a_1 * a_1) - 2 * a_2 / (a_1 * a_0) - 2 * a_4 / (a_1 * a_2) - 2 * a_4 / (a_1 * a_2) + a_5 / (a_2 * a_2) + a_3 / (a_0 * a_2) - 2 * a_2 / (a_1 * a_0) + a_1 / (a_0 * a_0) + a_3 / (a_0 * a_2)\n",
    "    r_2 = 4 * a_4 / (a_1 * a_1) - 2 * a_3 / (a_1 * a_0) - 2 * a_5 / (a_1 * a_2) - 2 * a_5 / (a_1 * a_2) + a_6 / (a_2 * a_2) + a_4 / (a_0 * a_2) - 2 * a_3 / (a_1 * a_0) + a_2 / (a_0 * a_0) + a_4 / (a_0 * a_2)\n",
    "\n",
    "    # Calculate q and p\n",
    "    q_1 = 2 * a_2 / a_1 - a_1 / a_0 - a_3 / a_2\n",
    "    q_2 = 2 * a_3 / a_1 - a_2 / a_0 - a_4 / a_2\n",
    "    #p_0 = a_0\n",
    "    #p_1 = a_1\n",
    "    #p_2 = a_2\n",
    "\n",
    "    a = r_0 * r_1 * q_2 - 2 * r_0 * q_1 * r_2\n",
    "    b = a_0 * r_1 * r_2 - 2 * a_1 * r_0 * r_2 + a_2 * r_0 * r_1 - 2 * q_1 * q_2 * r_0\n",
    "    c = 3 * a_0 * r_1 * q_2 - 3 * r_0 * a_1 * q_2\n",
    "    d = 2 * a_0 * q_1 * q_2 + 2 * a_0 * r_1 * a_2 - a_0 * a_1 * r_2 - r_0 * a_1 * a_2\n",
    "    e = 2 * a_0 * q_1 * a_2 - a_0 * a_1 * q_2\n",
    "    lr = polynomial_solver(a,b,c,d,e)\n",
    "    g = a_1 + 2 * lr * q_1 + lr ** 2 * r_1\n",
    "    f = a_0 + lr ** 2 * r_0\n",
    "    h = a_2 + 2 * lr * q_2 + lr ** 2 * r_2\n",
    "    eigenness = g**2/(f*h)\n",
    "    n = torch.argmax(eigenness, dim=-1, keepdim=True)\n",
    "    lr = torch.gather(lr, -1, n)\n",
    "    return torch.gather(eigenness, -1, n), lr * (-x / a_0 + 2 * u / a_1 - v / a_2), n, lr\n",
    "    return torch.gather(eigenness, -1, n), (-lr / a_0) * x + (2 * lr / a_1) * u + (-lr / a_2) * v, n, lr\n",
    "\n",
    "\n",
    "def update_vector(x, v):\n",
    "    x = x + v\n",
    "    return x/torch.linalg.norm(x, dim = -1, keepdim = True)\n",
    "\n",
    "def grad_ascend_lr(A,x,threshold,steps_already,steps_max):\n",
    "    e,v,i,l = optimal_lr(A, x)\n",
    "    x = update_vector(x, v)\n",
    "    steps = steps_already + 1\n",
    "    cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    while cond.any():\n",
    "        steps = torch.where(cond, steps + 1, steps)\n",
    "        e,v,i,l = optimal_lr(A, x)\n",
    "        x = torch.where(cond, update_vector(x, v), x)\n",
    "        cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    v=[]\n",
    "    f=A(x)\n",
    "    #print_gpu_memory()\n",
    "    return x, torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1).unsqueeze(-1)**2, (\n",
    "            f-(torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1)).unsqueeze(-1)**2*x\n",
    "            )/torch.linalg.norm(f, dim=-1).unsqueeze(-1), steps\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "class square(nn.Module):\n",
    "    def __init__(self, g):\n",
    "        super(square, self).__init__()\n",
    "        self.g=g\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.g(self.g(x))\n",
    "\n",
    "class minus_f(nn.Module):\n",
    "    def __init__(self, g, f):\n",
    "        super(minus_f, self).__init__()\n",
    "        self.g=g\n",
    "        self.f=f\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.g(x) - self.f*x\n",
    "\n",
    "class t_minus(nn.Module):\n",
    "    def __init__(self, g, t):\n",
    "        super(t_minus, self).__init__()\n",
    "        self.g=g\n",
    "        self.t=t\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.t*x - self.g(x)\n",
    "\n",
    "\n",
    "class modified(nn.Module):\n",
    "    def __init__(self,expr,A,batch_size):\n",
    "        super(modified, self).__init__()\n",
    "        self.parts=[]\n",
    "        for arg in expr.args:\n",
    "            self.parts.append(modified(arg,A,batch_size))\n",
    "        if expr.is_Number:\n",
    "            self.param = nn.Parameter(torch.random.uniform((batch_size,1))*float(expr))\n",
    "        self.f=torch.rand(batch_size,1).cuda()\n",
    "        self.t=((torch.rand(batch_size,1).cuda())*torch.max(torch.cat([self.f**2, (1-self.f)**2], dim=1), dim=1, keepdim = True)[0]).cuda()\n",
    "        self.z=torch.max(torch.cat([self.t**2, (self.t-self.f**2)**2, (self.t-(1-self.f)**2)**2], dim=1), dim=1, keepdim = True)[0]\n",
    "        if expr == sympy.Symbol('t'):\n",
    "            self.A=A\n",
    "        self.expr=expr\n",
    "        self.minus_f = minus_f(self.A, self.f)\n",
    "        self.square_1 = square(self.minus_f)\n",
    "        self.t_minus = t_minus(self.square_1, self.t)\n",
    "        self.square_2 = square(self.t_minus)\n",
    "    \n",
    "    def forward_1(self,x):\n",
    "        return x - self.square_2(x)/self.z\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.forward_1(x)\n",
    "        y = self.forward_1(y)\n",
    "        y = self.forward_1(y)\n",
    "        return self.forward_1(y) \n",
    "        if self.expr == sympy.Symbol('t'):\n",
    "            return self.A(x)\n",
    "\n",
    "        if self.expr.is_Number:\n",
    "            return self.param*x\n",
    "\n",
    "        if self.expr.is_Add:\n",
    "            # Initialize res as float zeros to avoid dtype conflicts\n",
    "            res = torch.zeros_like(x)\n",
    "            for arg in self.parts:\n",
    "                res += arg(x)\n",
    "            return res\n",
    "\n",
    "        if self.expr.is_Mul:\n",
    "            # product: to mimic polynomial multiplication\n",
    "            for arg in self.parts:\n",
    "                x = arg(x)\n",
    "            return x\n",
    "\n",
    "        if self.expr.is_Pow:\n",
    "            for _ in range(self.parts[1]):\n",
    "                x = self.parts[0](x)\n",
    "            return x\n",
    " \n",
    "\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "batch_size=16384\n",
    "matrix_size=128\n",
    "A=torch.diag(1/(1+torch.tensor(range(matrix_size), dtype=torch.float64)))\n",
    "\n",
    "import scipy\n",
    "\n",
    "C = torch.tensor(scipy.stats.ortho_group.rvs(dim=matrix_size), dtype=torch.float64)\n",
    "A = torch.linalg.inv(C) @ A @ C\n",
    "\n",
    "f=nn.Linear(matrix_size,matrix_size, bias=False).cuda()\n",
    "f.weight=nn.Parameter(A.cuda(), requires_grad=False)\n",
    "#f.bias=nn.Parameter(torch.zeros_like(f.bias))\n",
    "\n",
    "h = nn.Linear(matrix_size, matrix_size, bias=False).cuda()\n",
    "h.weight=nn.Parameter(C.cuda(), requires_grad=False)\n",
    "\n",
    "probs = 0.5*torch.ones((batch_size, 1)).cuda()\n",
    "delta = torch.tensor(1e-6).cuda()\n",
    "\n",
    "class truncated(nn.Module):\n",
    "    def __init__(self, f, x):\n",
    "        super(truncated, self).__init__()\n",
    "        \n",
    "        y = f(x)\n",
    "        xx = torch.sum(x**2, dim = -1, keepdim = True)\n",
    "        xAx = torch.sum(x*y, dim = -1, keepdim = True)\n",
    "        xAAx = torch.sum(y**2, dim = -1, keepdim = True)\n",
    "        mean = xAx/xx\n",
    "        sigma = torch.sqrt(xAAx*xx - xAx**2)/xx\n",
    "        self.mean = mean + (-1)**torch.bernoulli(probs) * sigma\n",
    "        self.sigma = torch.sqrt(-2*sigma*torch.log(delta))\n",
    "        self.lambda_min = self.mean - self.sigma\n",
    "        self.lambda_max = self.mean + self.sigma\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.f(x) - self.mean * x\n",
    "        y = self.f(y) - self.mean * y\n",
    "        return x - y / torch.min(torch.cat((\n",
    "            (self.lambda_min - self.mean)**2, \n",
    "            (self.lambda_max - self.mean)**2), \n",
    "                dim = -1), dim = -1, keepdim = True)[0]\n",
    "\n",
    "result=np.zeros((matrix_size,))\n",
    "expr_1=t\n",
    "print(expr_1)\n",
    "t=[1]\n",
    "stepses=[]\n",
    "with torch.cuda.device(0):\n",
    "    for i in range(1):\n",
    "        x = torch.empty((batch_size,matrix_size), dtype = torch.float64).normal_(mean=0,std=1).cuda()\n",
    "        steps = torch.zeros((batch_size,1)).cuda()\n",
    "        t[0] = x/torch.linalg.norm(x, dim=-1, keepdim=True)\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim=-1, keepdim=True)[0]<0.9)\n",
    "        while torch.any((steps<1000) & cond):\n",
    "            g = modified(expr_1,f,batch_size)\n",
    "            s = grad_ascend_lr(g,t[0],1e-6,steps,1000)\n",
    "            t[0] = torch.where(cond, s[0], t[0])\n",
    "            steps = torch.where(cond, s[-1], steps)\n",
    "            s = [1]\n",
    "            cond = (torch.max(torch.abs(h(t[0])), dim=-1, keepdim=True)[0]<0.9)\n",
    "            print(i, list(np.where(torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=0).cpu().numpy()>0)[0]), end='\\r')\n",
    "        result+= torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=0).cpu().numpy()\n",
    "        print(i, list(np.where(result>0)[0]), end='\\r')\n",
    "print('\\n'+np.sum(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02f718-ea1d-40d8-8461-96ab6f16b40d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9098782-24c8-442f-ba9f-512f63f75ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.824e+03 2.549e+03 1.187e+03 1.052e+03 9.570e+02 7.230e+02 5.510e+02\n",
      " 4.090e+02 2.310e+02 1.750e+02 1.230e+02 7.700e+01 5.900e+01 2.900e+01\n",
      " 2.500e+01 1.500e+01 1.200e+01 1.300e+01 5.000e+00 1.000e+00 5.000e+00\n",
      " 5.000e+00 2.000e+00 2.000e+00 0.000e+00 4.000e+00 2.000e+00 0.000e+00\n",
      " 1.000e+00 2.000e+00 4.000e+00 0.000e+00 1.000e+00 0.000e+00 2.000e+00\n",
      " 1.000e+00 0.000e+00 2.000e+00 1.000e+00 3.000e+00 0.000e+00 2.000e+00\n",
      " 0.000e+00 0.000e+00 1.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd12bc38-f9b3-4db8-8302-d6a7a65e25f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 3., 3., 3.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(steps.cpu().numpy()[np.where(torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=1,keepdim=True).cpu().numpy()>0)])[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "067a244c-6e0b-4a81-95a4-a39edb837037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14014/559006909.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 30, 31, 32, 33, 34, 35, 37, 38, 40, 41, 43, 49, 55, 57] 0.6531982421875\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sympy\n",
    "from sympy import Symbol, Mul, Pow, Add\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Prints the current allocated, reserved, and free memory on the GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "        free_memory = torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device)  # Free memory in reserved area\n",
    "        free_memory_mb = free_memory / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "        print(f\"Allocated memory: {allocated_memory:.2f} MB\")\n",
    "        print(f\"Reserved memory: {reserved_memory:.2f} MB\")\n",
    "        print(f\"Free memory: {free_memory_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "t=sympy.Symbol('t')\n",
    "sqrt_3=torch.sqrt(torch.tensor([3.])).cuda()\n",
    "\n",
    "\n",
    "def complex_cbrt(z):\n",
    "    \"\"\"Calculate the cubic root of a complex number.\"\"\"\n",
    "    # Calculate the magnitude and angle of the complex number\n",
    "    r = z.abs()  # Magnitude\n",
    "    theta = torch.angle(z)  # Angle in radians\n",
    "    #print(\"z\")\n",
    "    #print(z)\n",
    "    #print(\"r\")\n",
    "    #print(r)\n",
    "    #print(\"theta\")\n",
    "    #print(theta)\n",
    "\n",
    "    # Calculate the cubic root of the magnitude and the angle divided by 3\n",
    "    r = r ** (1/3)  # Magnitude of the cubic root\n",
    "    theta = theta / 3  # Angle of the cubic root\n",
    "    #print(\"root_magnitude\")\n",
    "    #print(r)\n",
    "    #print(\"root_angle\")\n",
    "    #print(theta)\n",
    "\n",
    "    return r * torch.cos(theta) + 1j * r * torch.sin(theta)  # Return as a complex number\n",
    "\n",
    "def quartic_solver(a,b,c,d,e):\n",
    "    e = e/a\n",
    "    d = d/a\n",
    "    c = c/a\n",
    "    b = b/a\n",
    "    i, lr_1 = torch.frexp(e)\n",
    "    i, lr_2 = torch.frexp(d)\n",
    "    i, lr_3 = torch.frexp(c)\n",
    "    i, lr_4 = torch.frexp(b)\n",
    "    i = torch.max(\n",
    "        torch.cat((2 + 3 * torch.max(torch.cat((2 * lr_3, 2 + lr_4 + lr_2, 4 + lr_1), \n",
    "                                           dim = -1), dim = -1, keepdim = True)[0], \n",
    "                  2 * torch.max(torch.cat((1 + 3 * lr_3, 3 + lr_4 + lr_3 + lr_2, 5 + 2 * lr_4 + lr_1, 5 + 2 * lr_2, 8 + lr_3 + lr_1), \n",
    "                                        dim = -1), dim = -1, keepdim = True)[0]), \n",
    "                  dim = -1), dim = -1, keepdim = True)[0]//12\n",
    "        \n",
    "    a = c ** 2 - 3 * b * d + 12 * e\n",
    "    e = 2 * c ** 3 - 9 * b * c * d + 27 * b ** 2 * e + 27 * d ** 2 - 72 * c * e\n",
    "    e = torch.ldexp(e, -6*i)\n",
    "    a = torch.ldexp(a, -4*i)\n",
    "    #print(\"Delta_0\")\n",
    "    #print(a)\n",
    "    #print(\"Delta_1\")\n",
    "    #print(e)\n",
    "    #print(\"under sqrt\")\n",
    "    #print(e ** 2 - 4 * a ** 3)\n",
    "    #print(\"sqrt\")\n",
    "    #print(torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64)))\n",
    "\n",
    "    Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2) \n",
    "    #print(\"Q\")\n",
    "    #print(Q)\n",
    "    \n",
    "    # Calculate p and q\n",
    "    e = torch.ldexp(c - 0.375 * b ** 2, -2*i)\n",
    "    d = torch.ldexp((0.5*b) ** 3 - 0.5 * b * c + d, -3*i)\n",
    "    #print(\"q\")\n",
    "    #print(d)\n",
    "    S = torch.sqrt(-2 / 3 * e + (Q + a / Q) / 3) / 2\n",
    "    a = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e + d / S)\n",
    "    c = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e - d / S)\n",
    "    b = torch.ldexp(-b, -i-2)\n",
    "\n",
    "    lr_1 = torch.ldexp(torch.real(b - S + a), i)\n",
    "    lr_2 = torch.ldexp(torch.real(b - S - a), i)\n",
    "    lr_3 = torch.ldexp(torch.real(b + S + c), i)\n",
    "    lr_4 = torch.ldexp(torch.real(b + S - c), i)\n",
    "    \n",
    "    return torch.cat([lr_1, lr_2, lr_3, lr_4],dim = -1)\n",
    "\n",
    "def cubic_solver(a,b,c,d):\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    a = c - b ** 2 / 3\n",
    "    d = 2/27*b**3 - b*c/3 + d\n",
    "    c = (a/3)**3 + (d/2)**2\n",
    "    c = torch.sqrt(c)\n",
    "    alpha = -d/2 + c\n",
    "    beta = -d/2 - c\n",
    "    alpha = complex_cbrt(alpha)\n",
    "    beta = complex_cbrt(beta)\n",
    "    \n",
    "    lr1 = torch.real(alpha+beta)\n",
    "    lr2 = torch.real((-(alpha+beta) + 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    lr3 = torch.real((-(alpha+beta) - 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    return torch.cat([lr1, lr2, lr3, torch.zeros_like(lr1)],dim=-1)\n",
    "\n",
    "\n",
    "def quadratic_solver(c,d,e):\n",
    "    D = torch.sqrt(d ** 2 - 4 * e * c)\n",
    "    x1 = torch.real((-d + D)/(2*c))\n",
    "    x2 = torch.real((-d - D)/(2*c))\n",
    "    return torch.cat([x1, x2, torch.zeros_like(x1), torch.zeros_like(x1)], dim=-1)\n",
    "\n",
    "def polynomial_solver(a,b,c,d,e):\n",
    "    return torch.where(a!=0, \n",
    "                       quartic_solver(a,b,c,d,e), torch.where(\n",
    "                        b!=0,\n",
    "                        cubic_solver(b,c,d,e), torch.where(\n",
    "                        c!=0,\n",
    "                        quadratic_solver(c,d,e), torch.where(\n",
    "                        d!=0, \n",
    "                        (e/d).repeat((len(a.shape)-1)*(1,)+(4,)), \n",
    "                        e.repeat((len(a.shape)-1)*(1,)+(4,))))))\n",
    "\n",
    "\n",
    "def optimal_lr(A, x):\n",
    "    \"\"\"Calculate optimal learning rate based on matrix A and vector x.\"\"\"\n",
    "    # Compute intermediate vectors\n",
    "    u = A(x)\n",
    "    v = A(u)\n",
    "    w = A(v)\n",
    "    \n",
    "    # Compute inner products\n",
    "    a_0 = torch.sum(x*x, dim = -1, keepdim = True)\n",
    "    a_1 = torch.sum(u*x, dim = -1, keepdim = True)\n",
    "    a_2 = torch.sum(v*x, dim = -1, keepdim = True)\n",
    "    a_3 = torch.sum(w*x, dim = -1, keepdim = True)\n",
    "    a_4 = torch.sum(w*u, dim = -1, keepdim = True)\n",
    "    a_5 = torch.sum(w*v, dim = -1, keepdim = True)\n",
    "    a_6 = torch.sum(w*w, dim = -1, keepdim = True)\n",
    "\n",
    "    # Calculate r_0, r_1, r_2\n",
    "    r_0 = 4 * a_2 / (a_1 * a_1) - 2 * a_1 / (a_1 * a_0) - 2 * a_3 / (a_1 * a_2) - 2 * a_3 / (a_1 * a_2) + a_4 / (a_2 * a_2) + a_2 / (a_0 * a_2) - 2 * a_1 / (a_1 * a_0) + a_0 / (a_0 * a_0) + a_2 / (a_0 * a_2)\n",
    "    r_1 = 4 * a_3 / (a_1 * a_1) - 2 * a_2 / (a_1 * a_0) - 2 * a_4 / (a_1 * a_2) - 2 * a_4 / (a_1 * a_2) + a_5 / (a_2 * a_2) + a_3 / (a_0 * a_2) - 2 * a_2 / (a_1 * a_0) + a_1 / (a_0 * a_0) + a_3 / (a_0 * a_2)\n",
    "    r_2 = 4 * a_4 / (a_1 * a_1) - 2 * a_3 / (a_1 * a_0) - 2 * a_5 / (a_1 * a_2) - 2 * a_5 / (a_1 * a_2) + a_6 / (a_2 * a_2) + a_4 / (a_0 * a_2) - 2 * a_3 / (a_1 * a_0) + a_2 / (a_0 * a_0) + a_4 / (a_0 * a_2)\n",
    "\n",
    "    # Calculate q and p\n",
    "    q_1 = 2 * a_2 / a_1 - a_1 / a_0 - a_3 / a_2\n",
    "    q_2 = 2 * a_3 / a_1 - a_2 / a_0 - a_4 / a_2\n",
    "    #p_0 = a_0\n",
    "    #p_1 = a_1\n",
    "    #p_2 = a_2\n",
    "\n",
    "    a = r_0 * r_1 * q_2 - 2 * r_0 * q_1 * r_2\n",
    "    b = a_0 * r_1 * r_2 - 2 * a_1 * r_0 * r_2 + a_2 * r_0 * r_1 - 2 * q_1 * q_2 * r_0\n",
    "    c = 3 * a_0 * r_1 * q_2 - 3 * r_0 * a_1 * q_2\n",
    "    d = 2 * a_0 * q_1 * q_2 + 2 * a_0 * r_1 * a_2 - a_0 * a_1 * r_2 - r_0 * a_1 * a_2\n",
    "    e = 2 * a_0 * q_1 * a_2 - a_0 * a_1 * q_2\n",
    "    lr = polynomial_solver(a,b,c,d,e)\n",
    "    g = a_1 + 2 * lr * q_1 + lr ** 2 * r_1\n",
    "    f = a_0 + lr ** 2 * r_0\n",
    "    h = a_2 + 2 * lr * q_2 + lr ** 2 * r_2\n",
    "    eigenness = g**2/(f*h)\n",
    "    n = torch.argmax(eigenness, dim=-1, keepdim=True)\n",
    "    lr = torch.gather(lr, -1, n)\n",
    "    return torch.gather(eigenness, -1, n), lr * (-x / a_0 + 2 * u / a_1 - v / a_2), n, lr\n",
    "    return torch.gather(eigenness, -1, n), (-lr / a_0) * x + (2 * lr / a_1) * u + (-lr / a_2) * v, n, lr\n",
    "\n",
    "\n",
    "def update_vector(x, v):\n",
    "    x = x + v\n",
    "    return x/torch.linalg.norm(x, dim = -1, keepdim = True)\n",
    "\n",
    "def grad_ascend_lr(A,x,threshold,steps_already,steps_max):\n",
    "    e,v,i,l = optimal_lr(A, x)\n",
    "    x = update_vector(x, v)\n",
    "    steps = steps_already + 1\n",
    "    cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    while cond.any():\n",
    "        steps = torch.where(cond, steps + 1, steps)\n",
    "        e,v,i,l = optimal_lr(A, x)\n",
    "        x = torch.where(cond, update_vector(x, v), x)\n",
    "        cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    v=[]\n",
    "    f=A(x)\n",
    "    #print_gpu_memory()\n",
    "    return x, torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1).unsqueeze(-1)**2, (\n",
    "            f-(torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1)).unsqueeze(-1)**2*x\n",
    "            )/torch.linalg.norm(f, dim=-1).unsqueeze(-1), steps\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "class square(nn.Module):\n",
    "    def __init__(self, g):\n",
    "        super(square, self).__init__()\n",
    "        self.g=g\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.g(self.g(x))\n",
    "\n",
    "class minus_f(nn.Module):\n",
    "    def __init__(self, g, f):\n",
    "        super(minus_f, self).__init__()\n",
    "        self.g=g\n",
    "        self.f=f\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.g(x) - self.f*x\n",
    "\n",
    "class t_minus(nn.Module):\n",
    "    def __init__(self, g, t):\n",
    "        super(t_minus, self).__init__()\n",
    "        self.g=g\n",
    "        self.t=t\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.t*x - self.g(x)\n",
    "\n",
    "\n",
    "class modified(nn.Module):\n",
    "    def __init__(self,expr,A,batch_size):\n",
    "        super(modified, self).__init__()\n",
    "        self.parts=[]\n",
    "        for arg in expr.args:\n",
    "            self.parts.append(modified(arg,A,batch_size))\n",
    "        if expr.is_Number:\n",
    "            self.param = nn.Parameter(torch.random.uniform((batch_size,1))*float(expr))\n",
    "        self.f=torch.rand(batch_size,1).cuda()\n",
    "        self.t=((torch.rand(batch_size,1).cuda())*torch.max(torch.cat([self.f**2, (1-self.f)**2], dim=1), dim=1, keepdim = True)[0]).cuda()\n",
    "        self.z=torch.max(torch.cat([self.t**2, (self.t-self.f**2)**2, (self.t-(1-self.f)**2)**2], dim=1), dim=1, keepdim = True)[0]\n",
    "        if expr == sympy.Symbol('t'):\n",
    "            self.A=A\n",
    "        self.expr=expr\n",
    "        self.minus_f = minus_f(self.A, self.f)\n",
    "        self.square_1 = square(self.minus_f)\n",
    "        self.t_minus = t_minus(self.square_1, self.t)\n",
    "        self.square_2 = square(self.t_minus)\n",
    "    \n",
    "    def forward_1(self,x):\n",
    "        return x - self.square_2(x)/self.z\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.forward_1(x)\n",
    "        y = self.forward_1(y)\n",
    "        y = self.forward_1(y)\n",
    "        return self.forward_1(y) \n",
    "        if self.expr == sympy.Symbol('t'):\n",
    "            return self.A(x)\n",
    "\n",
    "        if self.expr.is_Number:\n",
    "            return self.param*x\n",
    "\n",
    "        if self.expr.is_Add:\n",
    "            # Initialize res as float zeros to avoid dtype conflicts\n",
    "            res = torch.zeros_like(x)\n",
    "            for arg in self.parts:\n",
    "                res += arg(x)\n",
    "            return res\n",
    "\n",
    "        if self.expr.is_Mul:\n",
    "            # product: to mimic polynomial multiplication\n",
    "            for arg in self.parts:\n",
    "                x = arg(x)\n",
    "            return x\n",
    "\n",
    "        if self.expr.is_Pow:\n",
    "            for _ in range(self.parts[1]):\n",
    "                x = self.parts[0](x)\n",
    "            return x\n",
    " \n",
    "\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "batch_size=16384\n",
    "matrix_size=128\n",
    "A=torch.diag(1/(1+torch.tensor(range(matrix_size), dtype=torch.float64)))\n",
    "\n",
    "import scipy\n",
    "\n",
    "C = torch.tensor(scipy.stats.ortho_group.rvs(dim=matrix_size), dtype=torch.float64)\n",
    "A = torch.linalg.inv(C) @ A @ C\n",
    "\n",
    "f=nn.Linear(matrix_size,matrix_size, bias=False).cuda()\n",
    "f.weight=nn.Parameter(A.cuda(), requires_grad=False)\n",
    "#f.bias=nn.Parameter(torch.zeros_like(f.bias))\n",
    "\n",
    "h = nn.Linear(matrix_size, matrix_size, bias=False).cuda()\n",
    "h.weight=nn.Parameter(C.cuda(), requires_grad=False)\n",
    "\n",
    "probs = 0.5*torch.ones((batch_size, 1)).cuda()\n",
    "delta = torch.tensor(1e-6).cuda()\n",
    "\n",
    "class truncated(nn.Module):\n",
    "    def __init__(self, f, x):\n",
    "        super(truncated, self).__init__()\n",
    "        \n",
    "        y = f(x)\n",
    "        xx = torch.sum(x**2, dim = -1, keepdim = True)\n",
    "        xAx = torch.sum(x*y, dim = -1, keepdim = True)\n",
    "        xAAx = torch.sum(y**2, dim = -1, keepdim = True)\n",
    "        mean = xAx/xx\n",
    "        sigma = torch.sqrt(xAAx*xx - xAx**2)/xx\n",
    "        self.mean = mean + (-1)**torch.bernoulli(probs) * sigma\n",
    "        self.sigma = torch.sqrt(-2*sigma*torch.log(delta))\n",
    "        self.lambda_min = self.mean - self.sigma\n",
    "        self.lambda_max = self.mean + self.sigma\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.f(x) - self.mean * x\n",
    "        y = self.f(y) - self.mean * y\n",
    "        return x - y / torch.min(torch.cat((\n",
    "            (self.lambda_min - self.mean)**2, \n",
    "            (self.lambda_max - self.mean)**2), \n",
    "                dim = -1), dim = -1, keepdim = True)[0]\n",
    "\n",
    "result=np.zeros((matrix_size,))\n",
    "expr_1=t\n",
    "print(expr_1)\n",
    "t=[1]\n",
    "stepses=[]\n",
    "with torch.cuda.device(0):\n",
    "    for i in range(1):\n",
    "        x = torch.empty((batch_size, matrix_size), dtype=torch.float64).normal_(mean = 0, std = 1).cuda()\n",
    "        steps = torch.zeros((batch_size, 1)).cuda()\n",
    "        stepses.append(torch.zeros((batch_size, 1)).cuda())\n",
    "        t[0] = x/torch.linalg.norm(x, dim = -1, keepdim = True)\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "        g = modified(expr_1, f, batch_size)\n",
    "        s = grad_ascend_lr(g, t[0], 1e-6, steps, 1000)\n",
    "        t[0] = torch.where(cond, s[0], t[0])\n",
    "        steps = s[-1]\n",
    "        stepses[i] = torch.where(cond, s[-1], stepses[i])\n",
    "        s = [1]\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "        g = truncated(f, t[0])\n",
    "        while torch.any((steps < 1000) & cond):\n",
    "            s = grad_ascend_lr(g, t[0], 1e-10, steps, 1000)\n",
    "            t[0] = torch.where(cond, s[0], t[0])\n",
    "            steps = s[-1]\n",
    "            stepses[i] = torch.where(cond, s[-1], stepses[i])\n",
    "            s = [1]\n",
    "            cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "            g = truncated(f, t[0])\n",
    "        result+= torch.sum(torch.where(torch.abs(h(t[0])) >= 0.9, 1, 0), dim = 0).cpu().numpy()\n",
    "        print(i, list(np.where(result > 0)[0]), np.sum(result) / ((i + 1) * batch_size), end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fdfb03f-b19e-45ad-a0d9-9ed1f5f174ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14.,  16.,  17., ..., 103., 103., 103.], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(steps.cpu().numpy()[np.where(torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=1,keepdim=True).cpu().numpy()>0)])[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb7180e7-b976-4e01-b504-b2395a99d777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.711e+03 1.675e+03 8.690e+02 8.170e+02 6.890e+02 5.580e+02 4.200e+02\n",
      " 3.280e+02 2.020e+02 1.040e+02 8.800e+01 5.900e+01 4.400e+01 2.700e+01\n",
      " 2.100e+01 1.900e+01 1.400e+01 4.000e+00 9.000e+00 3.000e+00 3.000e+00\n",
      " 3.000e+00 1.000e+00 2.000e+00 5.000e+00 6.000e+00 0.000e+00 2.000e+00\n",
      " 1.000e+00 0.000e+00 1.000e+00 2.000e+00 3.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 0.000e+00 1.000e+00 2.000e+00 0.000e+00 1.000e+00 1.000e+00\n",
      " 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      " 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89f60497-3e0e-4432-ae80-e4bcc9910a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14014/3199576586.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 56, 57, 58, 60, 61, 63, 70, 71, 75, 127] 0.62646484375\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sympy\n",
    "from sympy import Symbol, Mul, Pow, Add\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Prints the current allocated, reserved, and free memory on the GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "        free_memory = torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device)  # Free memory in reserved area\n",
    "        free_memory_mb = free_memory / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "        print(f\"Allocated memory: {allocated_memory:.2f} MB\")\n",
    "        print(f\"Reserved memory: {reserved_memory:.2f} MB\")\n",
    "        print(f\"Free memory: {free_memory_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "t=sympy.Symbol('t')\n",
    "sqrt_3=torch.sqrt(torch.tensor([3.])).cuda()\n",
    "\n",
    "\n",
    "def complex_cbrt(z):\n",
    "    \"\"\"Calculate the cubic root of a complex number.\"\"\"\n",
    "    # Calculate the magnitude and angle of the complex number\n",
    "    r = z.abs()  # Magnitude\n",
    "    theta = torch.angle(z)  # Angle in radians\n",
    "    #print(\"z\")\n",
    "    #print(z)\n",
    "    #print(\"r\")\n",
    "    #print(r)\n",
    "    #print(\"theta\")\n",
    "    #print(theta)\n",
    "\n",
    "    # Calculate the cubic root of the magnitude and the angle divided by 3\n",
    "    r = r ** (1/3)  # Magnitude of the cubic root\n",
    "    theta = theta / 3  # Angle of the cubic root\n",
    "    #print(\"root_magnitude\")\n",
    "    #print(r)\n",
    "    #print(\"root_angle\")\n",
    "    #print(theta)\n",
    "\n",
    "    return r * torch.cos(theta) + 1j * r * torch.sin(theta)  # Return as a complex number\n",
    "\n",
    "def quartic_solver(a,b,c,d,e):\n",
    "    e = e/a\n",
    "    d = d/a\n",
    "    c = c/a\n",
    "    b = b/a\n",
    "    i, lr_1 = torch.frexp(e)\n",
    "    i, lr_2 = torch.frexp(d)\n",
    "    i, lr_3 = torch.frexp(c)\n",
    "    i, lr_4 = torch.frexp(b)\n",
    "    i = torch.max(\n",
    "        torch.cat((2 + 3 * torch.max(torch.cat((2 * lr_3, 2 + lr_4 + lr_2, 4 + lr_1), \n",
    "                                           dim = -1), dim = -1, keepdim = True)[0], \n",
    "                  2 * torch.max(torch.cat((1 + 3 * lr_3, 3 + lr_4 + lr_3 + lr_2, 5 + 2 * lr_4 + lr_1, 5 + 2 * lr_2, 8 + lr_3 + lr_1), \n",
    "                                        dim = -1), dim = -1, keepdim = True)[0]), \n",
    "                  dim = -1), dim = -1, keepdim = True)[0]//12\n",
    "        \n",
    "    a = c ** 2 - 3 * b * d + 12 * e\n",
    "    e = 2 * c ** 3 - 9 * b * c * d + 27 * b ** 2 * e + 27 * d ** 2 - 72 * c * e\n",
    "    e = torch.ldexp(e, -6*i)\n",
    "    a = torch.ldexp(a, -4*i)\n",
    "    #print(\"Delta_0\")\n",
    "    #print(a)\n",
    "    #print(\"Delta_1\")\n",
    "    #print(e)\n",
    "    #print(\"under sqrt\")\n",
    "    #print(e ** 2 - 4 * a ** 3)\n",
    "    #print(\"sqrt\")\n",
    "    #print(torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64)))\n",
    "\n",
    "    Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2) \n",
    "    #print(\"Q\")\n",
    "    #print(Q)\n",
    "    \n",
    "    # Calculate p and q\n",
    "    e = torch.ldexp(c - 0.375 * b ** 2, -2*i)\n",
    "    d = torch.ldexp((0.5*b) ** 3 - 0.5 * b * c + d, -3*i)\n",
    "    #print(\"q\")\n",
    "    #print(d)\n",
    "    S = torch.sqrt(-2 / 3 * e + (Q + a / Q) / 3) / 2\n",
    "    a = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e + d / S)\n",
    "    c = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e - d / S)\n",
    "    b = torch.ldexp(-b, -i-2)\n",
    "\n",
    "    lr_1 = torch.ldexp(torch.real(b - S + a), i)\n",
    "    lr_2 = torch.ldexp(torch.real(b - S - a), i)\n",
    "    lr_3 = torch.ldexp(torch.real(b + S + c), i)\n",
    "    lr_4 = torch.ldexp(torch.real(b + S - c), i)\n",
    "    \n",
    "    return torch.cat([lr_1, lr_2, lr_3, lr_4],dim = -1)\n",
    "\n",
    "def cubic_solver(a,b,c,d):\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    a = c - b ** 2 / 3\n",
    "    d = 2/27*b**3 - b*c/3 + d\n",
    "    c = (a/3)**3 + (d/2)**2\n",
    "    c = torch.sqrt(c)\n",
    "    alpha = -d/2 + c\n",
    "    beta = -d/2 - c\n",
    "    alpha = complex_cbrt(alpha)\n",
    "    beta = complex_cbrt(beta)\n",
    "    \n",
    "    lr1 = torch.real(alpha+beta)\n",
    "    lr2 = torch.real((-(alpha+beta) + 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    lr3 = torch.real((-(alpha+beta) - 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    return torch.cat([lr1, lr2, lr3, torch.zeros_like(lr1)],dim=-1)\n",
    "\n",
    "\n",
    "def quadratic_solver(c,d,e):\n",
    "    D = torch.sqrt(d ** 2 - 4 * e * c)\n",
    "    x1 = torch.real((-d + D)/(2*c))\n",
    "    x2 = torch.real((-d - D)/(2*c))\n",
    "    return torch.cat([x1, x2, torch.zeros_like(x1), torch.zeros_like(x1)], dim=-1)\n",
    "\n",
    "def polynomial_solver(a,b,c,d,e):\n",
    "    return torch.where(a!=0, \n",
    "                       quartic_solver(a,b,c,d,e), torch.where(\n",
    "                        b!=0,\n",
    "                        cubic_solver(b,c,d,e), torch.where(\n",
    "                        c!=0,\n",
    "                        quadratic_solver(c,d,e), torch.where(\n",
    "                        d!=0, \n",
    "                        (e/d).repeat((len(a.shape)-1)*(1,)+(4,)), \n",
    "                        e.repeat((len(a.shape)-1)*(1,)+(4,))))))\n",
    "\n",
    "\n",
    "def optimal_lr(A, x):\n",
    "    \"\"\"Calculate optimal learning rate based on matrix A and vector x.\"\"\"\n",
    "    # Compute intermediate vectors\n",
    "    u = A(x)\n",
    "    v = A(u)\n",
    "    w = A(v)\n",
    "    \n",
    "    # Compute inner products\n",
    "    a_0 = torch.sum(x*x, dim = -1, keepdim = True)\n",
    "    a_1 = torch.sum(u*x, dim = -1, keepdim = True)\n",
    "    a_2 = torch.sum(v*x, dim = -1, keepdim = True)\n",
    "    a_3 = torch.sum(w*x, dim = -1, keepdim = True)\n",
    "    a_4 = torch.sum(w*u, dim = -1, keepdim = True)\n",
    "    a_5 = torch.sum(w*v, dim = -1, keepdim = True)\n",
    "    a_6 = torch.sum(w*w, dim = -1, keepdim = True)\n",
    "\n",
    "    # Calculate r_0, r_1, r_2\n",
    "    r_0 = 4 * a_2 / (a_1 * a_1) - 2 * a_1 / (a_1 * a_0) - 2 * a_3 / (a_1 * a_2) - 2 * a_3 / (a_1 * a_2) + a_4 / (a_2 * a_2) + a_2 / (a_0 * a_2) - 2 * a_1 / (a_1 * a_0) + a_0 / (a_0 * a_0) + a_2 / (a_0 * a_2)\n",
    "    r_1 = 4 * a_3 / (a_1 * a_1) - 2 * a_2 / (a_1 * a_0) - 2 * a_4 / (a_1 * a_2) - 2 * a_4 / (a_1 * a_2) + a_5 / (a_2 * a_2) + a_3 / (a_0 * a_2) - 2 * a_2 / (a_1 * a_0) + a_1 / (a_0 * a_0) + a_3 / (a_0 * a_2)\n",
    "    r_2 = 4 * a_4 / (a_1 * a_1) - 2 * a_3 / (a_1 * a_0) - 2 * a_5 / (a_1 * a_2) - 2 * a_5 / (a_1 * a_2) + a_6 / (a_2 * a_2) + a_4 / (a_0 * a_2) - 2 * a_3 / (a_1 * a_0) + a_2 / (a_0 * a_0) + a_4 / (a_0 * a_2)\n",
    "\n",
    "    # Calculate q and p\n",
    "    q_1 = 2 * a_2 / a_1 - a_1 / a_0 - a_3 / a_2\n",
    "    q_2 = 2 * a_3 / a_1 - a_2 / a_0 - a_4 / a_2\n",
    "    #p_0 = a_0\n",
    "    #p_1 = a_1\n",
    "    #p_2 = a_2\n",
    "\n",
    "    a = r_0 * r_1 * q_2 - 2 * r_0 * q_1 * r_2\n",
    "    b = a_0 * r_1 * r_2 - 2 * a_1 * r_0 * r_2 + a_2 * r_0 * r_1 - 2 * q_1 * q_2 * r_0\n",
    "    c = 3 * a_0 * r_1 * q_2 - 3 * r_0 * a_1 * q_2\n",
    "    d = 2 * a_0 * q_1 * q_2 + 2 * a_0 * r_1 * a_2 - a_0 * a_1 * r_2 - r_0 * a_1 * a_2\n",
    "    e = 2 * a_0 * q_1 * a_2 - a_0 * a_1 * q_2\n",
    "    lr = polynomial_solver(a,b,c,d,e)\n",
    "    g = a_1 + 2 * lr * q_1 + lr ** 2 * r_1\n",
    "    f = a_0 + lr ** 2 * r_0\n",
    "    h = a_2 + 2 * lr * q_2 + lr ** 2 * r_2\n",
    "    eigenness = g**2/(f*h)\n",
    "    n = torch.argmax(eigenness, dim=-1, keepdim=True)\n",
    "    lr = torch.gather(lr, -1, n)\n",
    "    return torch.gather(eigenness, -1, n), lr * (-x / a_0 + 2 * u / a_1 - v / a_2), n, lr\n",
    "    return torch.gather(eigenness, -1, n), (-lr / a_0) * x + (2 * lr / a_1) * u + (-lr / a_2) * v, n, lr\n",
    "\n",
    "\n",
    "def update_vector(x, v):\n",
    "    x = x + v\n",
    "    return x/torch.linalg.norm(x, dim = -1, keepdim = True)\n",
    "\n",
    "def grad_ascend_lr(A,x,threshold,steps_already,steps_max):\n",
    "    e,v,i,l = optimal_lr(A, x)\n",
    "    x = update_vector(x, v)\n",
    "    steps = steps_already + 1\n",
    "    cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    while cond.any():\n",
    "        steps = torch.where(cond, steps + 1, steps)\n",
    "        e,v,i,l = optimal_lr(A, x)\n",
    "        x = torch.where(cond, update_vector(x, v), x)\n",
    "        cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    v=[]\n",
    "    f=A(x)\n",
    "    #print_gpu_memory()\n",
    "    return x, torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1).unsqueeze(-1)**2, (\n",
    "            f-(torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1)).unsqueeze(-1)**2*x\n",
    "            )/torch.linalg.norm(f, dim=-1).unsqueeze(-1), steps\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "class square(nn.Module):\n",
    "    def __init__(self, g):\n",
    "        super(square, self).__init__()\n",
    "        self.g=g\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.g(self.g(x))\n",
    "\n",
    "class minus_f(nn.Module):\n",
    "    def __init__(self, g, f):\n",
    "        super(minus_f, self).__init__()\n",
    "        self.g=g\n",
    "        self.f=f\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.g(x) - self.f*x\n",
    "\n",
    "class t_minus(nn.Module):\n",
    "    def __init__(self, g, t):\n",
    "        super(t_minus, self).__init__()\n",
    "        self.g=g\n",
    "        self.t=t\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.t*x - self.g(x)\n",
    "\n",
    "\n",
    "class modified(nn.Module):\n",
    "    def __init__(self,expr,A,batch_size):\n",
    "        super(modified, self).__init__()\n",
    "        self.parts=[]\n",
    "        for arg in expr.args:\n",
    "            self.parts.append(modified(arg,A,batch_size))\n",
    "        if expr.is_Number:\n",
    "            self.param = nn.Parameter(torch.random.uniform((batch_size,1))*float(expr))\n",
    "        self.f=torch.rand(batch_size,1).cuda()\n",
    "        self.t=((torch.rand(batch_size,1).cuda())*torch.max(torch.cat([self.f**2, (1-self.f)**2], dim=1), dim=1, keepdim = True)[0]).cuda()\n",
    "        self.z=torch.max(torch.cat([self.t**2, (self.t-self.f**2)**2, (self.t-(1-self.f)**2)**2], dim=1), dim=1, keepdim = True)[0]\n",
    "        if expr == sympy.Symbol('t'):\n",
    "            self.A=A\n",
    "        self.expr=expr\n",
    "        self.minus_f = minus_f(self.A, self.f)\n",
    "        self.square_1 = square(self.minus_f)\n",
    "        self.t_minus = t_minus(self.square_1, self.t)\n",
    "        self.square_2 = square(self.t_minus)\n",
    "    \n",
    "    def forward_1(self,x):\n",
    "        return x - self.square_2(x)/self.z\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.forward_1(x)\n",
    "        y = self.forward_1(y)\n",
    "        y = self.forward_1(y)\n",
    "        return self.forward_1(y) \n",
    "        if self.expr == sympy.Symbol('t'):\n",
    "            return self.A(x)\n",
    "\n",
    "        if self.expr.is_Number:\n",
    "            return self.param*x\n",
    "\n",
    "        if self.expr.is_Add:\n",
    "            # Initialize res as float zeros to avoid dtype conflicts\n",
    "            res = torch.zeros_like(x)\n",
    "            for arg in self.parts:\n",
    "                res += arg(x)\n",
    "            return res\n",
    "\n",
    "        if self.expr.is_Mul:\n",
    "            # product: to mimic polynomial multiplication\n",
    "            for arg in self.parts:\n",
    "                x = arg(x)\n",
    "            return x\n",
    "\n",
    "        if self.expr.is_Pow:\n",
    "            for _ in range(self.parts[1]):\n",
    "                x = self.parts[0](x)\n",
    "            return x\n",
    " \n",
    "\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "batch_size=16384\n",
    "matrix_size=128\n",
    "A=torch.diag(1/(1+torch.tensor(range(matrix_size), dtype=torch.float64)))\n",
    "\n",
    "import scipy\n",
    "\n",
    "C = torch.tensor(scipy.stats.ortho_group.rvs(dim=matrix_size), dtype=torch.float64)\n",
    "A = torch.linalg.inv(C) @ A @ C\n",
    "\n",
    "f=nn.Linear(matrix_size,matrix_size, bias=False).cuda()\n",
    "f.weight=nn.Parameter(A.cuda(), requires_grad=False)\n",
    "#f.bias=nn.Parameter(torch.zeros_like(f.bias))\n",
    "\n",
    "h = nn.Linear(matrix_size, matrix_size, bias=False).cuda()\n",
    "h.weight=nn.Parameter(C.cuda(), requires_grad=False)\n",
    "\n",
    "probs = 0.5*torch.ones((batch_size, 1)).cuda()\n",
    "delta = torch.tensor(1e-6).cuda()\n",
    "\n",
    "class truncated(nn.Module):\n",
    "    def __init__(self, f, x):\n",
    "        super(truncated, self).__init__()\n",
    "        \n",
    "        y = f(x)\n",
    "        xx = torch.sum(x**2, dim = -1, keepdim = True)\n",
    "        xAx = torch.sum(x*y, dim = -1, keepdim = True)\n",
    "        xAAx = torch.sum(y**2, dim = -1, keepdim = True)\n",
    "        mean = xAx/xx\n",
    "        sigma = torch.sqrt(xAAx*xx - xAx**2)/xx\n",
    "        self.mean = mean + (-1)**torch.bernoulli(probs) * sigma\n",
    "        self.sigma = torch.sqrt(-2*sigma*torch.log(delta))\n",
    "        self.lambda_min = self.mean - self.sigma\n",
    "        self.lambda_max = self.mean + self.sigma\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.f(x) - self.mean * x\n",
    "        y = self.f(y) - self.mean * y\n",
    "        return x - y / torch.min(torch.cat((\n",
    "            (self.lambda_min - self.mean)**2, \n",
    "            (self.lambda_max - self.mean)**2), \n",
    "                dim = -1), dim = -1, keepdim = True)[0]\n",
    "\n",
    "result=np.zeros((matrix_size,))\n",
    "expr_1=t\n",
    "print(expr_1)\n",
    "t=[1]\n",
    "stepses=[]\n",
    "with torch.cuda.device(0):\n",
    "    for i in range(1):\n",
    "        x = torch.empty((batch_size, matrix_size), dtype=torch.float64).normal_(mean = 0, std = 1).cuda()\n",
    "        steps = torch.zeros((batch_size, 1)).cuda()\n",
    "        stepses.append(torch.zeros((batch_size, 1)).cuda())\n",
    "        t[0] = x/torch.linalg.norm(x, dim = -1, keepdim = True)\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "        g = modified(expr_1, f, batch_size)\n",
    "        s = grad_ascend_lr(g, t[0], 1e-30, steps, 2000)\n",
    "        t[0] = torch.where(cond, s[0], t[0])\n",
    "        steps = s[-1]\n",
    "        stepses[i] = torch.where(cond, s[-1], stepses[i])\n",
    "        s = [1]\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "        g = truncated(f, t[0])\n",
    "        while torch.any((steps < 2000) & cond):\n",
    "            s = grad_ascend_lr(g, t[0], 1e-30, steps, 2000)\n",
    "            t[0] = torch.where(cond, s[0], t[0])\n",
    "            steps = s[-1]\n",
    "            stepses[i] = torch.where(cond, s[-1], stepses[i])\n",
    "            s = [1]\n",
    "            cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "            g = truncated(f, t[0])\n",
    "        result+= torch.sum(torch.where(torch.abs(h(t[0])) >= 0.9, 1, 0), dim = 0).cpu().numpy()\n",
    "        print(i, list(np.where(result > 0)[0]), np.sum(result) / ((i + 1) * batch_size), end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f08055b9-aac3-461d-9722-a58eade519cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   7.,    7.,    7.,    7.,    8.,    8.,    8.,    8.,    8.,\n",
       "          8.,    8.,    8.,    8.,    8.,    8.,    8.,    9.,    9.,\n",
       "          9.,    9.,    9.,    9.,    9.,   10.,   10.,   10.,   10.,\n",
       "         10.,   10.,   10.,   10.,   10.,   10.,   10.,   11.,   11.,\n",
       "         11.,   11.,   11.,   11.,   11.,   11.,   11.,   12.,   12.,\n",
       "         12.,   12.,   12.,   12.,   12.,   12.,   12.,   12.,   12.,\n",
       "         12.,   12.,   13.,   13.,   13.,   13.,   13.,   13.,   13.,\n",
       "         13.,   13.,   13.,   14.,   14.,   14.,   14.,   14.,   14.,\n",
       "         14.,   14.,   14.,   15.,   15.,   15.,   15.,   15.,   15.,\n",
       "         15.,   15.,   15.,   16.,   16.,   16.,   16.,   16.,   16.,\n",
       "         16.,   16.,   16.,   16.,   17.,   17.,   17.,   17.,   17.,\n",
       "         17.,   17.,   17.,   18.,   18.,   18.,   18.,   18.,   18.,\n",
       "         18.,   19.,   19.,   19.,   19.,   19.,   19.,   19.,   20.,\n",
       "         20.,   20.,   20.,   20.,   20.,   20.,   21.,   21.,   21.,\n",
       "         21.,   21.,   21.,   21.,   21.,   21.,   21.,   22.,   22.,\n",
       "         22.,   22.,   23.,   23.,   23.,   23.,   23.,   23.,   23.,\n",
       "         24.,   24.,   24.,   24.,   24.,   24.,   24.,   24.,   24.,\n",
       "         24.,   25.,   25.,   25.,   25.,   26.,   26.,   26.,   26.,\n",
       "         26.,   26.,   27.,   27.,   27.,   27.,   27.,   27.,   27.,\n",
       "         27.,   28.,   28.,   28.,   29.,   29.,   29.,   30.,   30.,\n",
       "         30.,   30.,   32.,   32.,   32.,   33.,   33.,   33.,   33.,\n",
       "         34.,   34.,   34.,   34.,   35.,   35.,   35.,   36.,   36.,\n",
       "         36.,   36.,   37.,   37.,   37.,   37.,   37.,   38.,   38.,\n",
       "         38.,   38.,   39.,   39.,   39.,   39.,   40.,   40.,   40.,\n",
       "         40.,   40.,   41.,   41.,   41.,   41.,   41.,   42.,   42.,\n",
       "         42.,   43.,   43.,   43.,   43.,   44.,   44.,   44.,   45.,\n",
       "         46.,   46.,   46.,   47.,   47.,   48.,   48.,   48.,   49.,\n",
       "         49.,   49.,   49.,   50.,   50.,   51.,   52.,   52.,   52.,\n",
       "         52.,   53.,   53.,   54.,   54.,   54.,   54.,   55.,   55.,\n",
       "         55.,   55.,   56.,   56.,   56.,   58.,   59.,   59.,   60.,\n",
       "         60.,   60.,   61.,   61.,   63.,   63.,   63.,   63.,   64.,\n",
       "         64.,   65.,   65.,   66.,   68.,   68.,   68.,   69.,   69.,\n",
       "         70.,   72.,   72.,   72.,   72.,   72.,   73.,   74.,   75.,\n",
       "         76.,   77.,   78.,   79.,   79.,   80.,   81.,   81.,   82.,\n",
       "         83.,   85.,   86.,   87.,   88.,   88.,   89.,   90.,   90.,\n",
       "         92.,   92.,   93.,   96.,   97.,  100.,  101.,  101.,  103.,\n",
       "        104.,  107.,  109.,  109.,  109.,  110.,  113.,  114.,  115.,\n",
       "        119.,  120.,  121.,  123.,  123.,  125.,  125.,  126.,  128.,\n",
       "        130.,  135.,  136.,  138.,  138.,  139.,  140.,  140.,  141.,\n",
       "        143.,  145.,  146.,  151.,  151.,  153.,  153.,  154.,  155.,\n",
       "        156.,  158.,  160.,  160.,  166.,  173.,  173.,  175.,  175.,\n",
       "        180.,  181.,  181.,  191.,  193.,  196.,  196.,  198.,  201.,\n",
       "        204.,  209.,  210.,  212.,  212.,  215.,  216.,  218.,  218.,\n",
       "        218.,  218.,  222.,  223.,  226.,  236.,  237.,  239.,  243.,\n",
       "        248.,  249.,  249.,  250.,  252.,  258.,  259.,  259.,  259.,\n",
       "        261.,  264.,  265.,  265.,  269.,  285.,  286.,  287.,  292.,\n",
       "        300.,  319.,  324.,  329.,  333.,  333.,  349.,  350.,  352.,\n",
       "        354.,  358.,  370.,  374.,  379.,  395.,  401.,  405.,  406.,\n",
       "        407.,  410.,  412.,  425.,  425.,  427.,  429.,  429.,  432.,\n",
       "        445.,  450.,  450.,  452.,  453.,  458.,  459.,  462.,  474.,\n",
       "        481.,  490.,  497.,  505.,  508.,  518.,  519.,  537.,  555.,\n",
       "        556.,  560.,  560.,  569.,  573.,  578.,  580.,  594.,  611.,\n",
       "        616.,  636.,  640.,  642.,  645.,  652.,  652.,  663.,  681.,\n",
       "        692.,  694.,  696.,  705.,  719.,  730.,  765.,  774.,  779.,\n",
       "        785.,  804.,  815.,  824.,  845.,  868.,  906.,  915.,  917.,\n",
       "        926.,  927.,  929.,  942.,  959.,  962.,  990.,  993., 1001.,\n",
       "       1006., 1029., 1042., 1044., 1044., 1046., 1072., 1086., 1091.,\n",
       "       1100., 1100., 1125., 1125., 1139., 1141., 1168., 1182., 1192.,\n",
       "       1193., 1193., 1194., 1206., 1221., 1223., 1232., 1241., 1241.,\n",
       "       1253., 1264., 1269., 1283., 1359., 1360., 1372., 1377., 1439.,\n",
       "       1446., 1475., 1485., 1496., 1501., 1502., 1518., 1527., 1529.,\n",
       "       1532., 1535., 1542., 1547., 1556., 1564., 1598., 1646., 1670.,\n",
       "       1751., 1825., 1838., 1847., 1899., 1910., 1930., 1937., 1950.,\n",
       "       1980., 1985., 1987., 1988., 2000., 2000., 2000., 2000., 2000.,\n",
       "       2000., 2000., 2000., 2000., 2000., 2000., 2000., 2000., 2000.,\n",
       "       2000., 2000., 2000., 2000., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001., 2001.,\n",
       "       2001.], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(steps.cpu().numpy()[np.where(torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=1,keepdim=True).cpu().numpy()>0)])[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02b4c8fb-b57e-4059-94ad-217ff7d4491b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10264.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf451fa3-d942-4278-9274-624c851ce590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14014/201636404.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 255] 0.61627197265625\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sympy\n",
    "from sympy import Symbol, Mul, Pow, Add\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Prints the current allocated, reserved, and free memory on the GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "        free_memory = torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device)  # Free memory in reserved area\n",
    "        free_memory_mb = free_memory / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "        print(f\"Allocated memory: {allocated_memory:.2f} MB\")\n",
    "        print(f\"Reserved memory: {reserved_memory:.2f} MB\")\n",
    "        print(f\"Free memory: {free_memory_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "t=sympy.Symbol('t')\n",
    "sqrt_3=torch.sqrt(torch.tensor([3.])).cuda()\n",
    "\n",
    "\n",
    "def complex_cbrt(z):\n",
    "    \"\"\"Calculate the cubic root of a complex number.\"\"\"\n",
    "    # Calculate the magnitude and angle of the complex number\n",
    "    r = z.abs()  # Magnitude\n",
    "    theta = torch.angle(z)  # Angle in radians\n",
    "    #print(\"z\")\n",
    "    #print(z)\n",
    "    #print(\"r\")\n",
    "    #print(r)\n",
    "    #print(\"theta\")\n",
    "    #print(theta)\n",
    "\n",
    "    # Calculate the cubic root of the magnitude and the angle divided by 3\n",
    "    r = r ** (1/3)  # Magnitude of the cubic root\n",
    "    theta = theta / 3  # Angle of the cubic root\n",
    "    #print(\"root_magnitude\")\n",
    "    #print(r)\n",
    "    #print(\"root_angle\")\n",
    "    #print(theta)\n",
    "\n",
    "    return r * torch.cos(theta) + 1j * r * torch.sin(theta)  # Return as a complex number\n",
    "\n",
    "def quartic_solver(a,b,c,d,e):\n",
    "    e = e/a\n",
    "    d = d/a\n",
    "    c = c/a\n",
    "    b = b/a\n",
    "    i, lr_1 = torch.frexp(e)\n",
    "    i, lr_2 = torch.frexp(d)\n",
    "    i, lr_3 = torch.frexp(c)\n",
    "    i, lr_4 = torch.frexp(b)\n",
    "    i = torch.max(\n",
    "        torch.cat((2 + 3 * torch.max(torch.cat((2 * lr_3, 2 + lr_4 + lr_2, 4 + lr_1), \n",
    "                                           dim = -1), dim = -1, keepdim = True)[0], \n",
    "                  2 * torch.max(torch.cat((1 + 3 * lr_3, 3 + lr_4 + lr_3 + lr_2, 5 + 2 * lr_4 + lr_1, 5 + 2 * lr_2, 8 + lr_3 + lr_1), \n",
    "                                        dim = -1), dim = -1, keepdim = True)[0]), \n",
    "                  dim = -1), dim = -1, keepdim = True)[0]//12\n",
    "        \n",
    "    a = c ** 2 - 3 * b * d + 12 * e\n",
    "    e = 2 * c ** 3 - 9 * b * c * d + 27 * b ** 2 * e + 27 * d ** 2 - 72 * c * e\n",
    "    e = torch.ldexp(e, -6*i)\n",
    "    a = torch.ldexp(a, -4*i)\n",
    "    #print(\"Delta_0\")\n",
    "    #print(a)\n",
    "    #print(\"Delta_1\")\n",
    "    #print(e)\n",
    "    #print(\"under sqrt\")\n",
    "    #print(e ** 2 - 4 * a ** 3)\n",
    "    #print(\"sqrt\")\n",
    "    #print(torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64)))\n",
    "\n",
    "    Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2) \n",
    "    #print(\"Q\")\n",
    "    #print(Q)\n",
    "    \n",
    "    # Calculate p and q\n",
    "    e = torch.ldexp(c - 0.375 * b ** 2, -2*i)\n",
    "    d = torch.ldexp((0.5*b) ** 3 - 0.5 * b * c + d, -3*i)\n",
    "    #print(\"q\")\n",
    "    #print(d)\n",
    "    S = torch.sqrt(-2 / 3 * e + (Q + a / Q) / 3) / 2\n",
    "    a = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e + d / S)\n",
    "    c = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e - d / S)\n",
    "    b = torch.ldexp(-b, -i-2)\n",
    "\n",
    "    lr_1 = torch.ldexp(torch.real(b - S + a), i)\n",
    "    lr_2 = torch.ldexp(torch.real(b - S - a), i)\n",
    "    lr_3 = torch.ldexp(torch.real(b + S + c), i)\n",
    "    lr_4 = torch.ldexp(torch.real(b + S - c), i)\n",
    "    \n",
    "    return torch.cat([lr_1, lr_2, lr_3, lr_4],dim = -1)\n",
    "\n",
    "def cubic_solver(a,b,c,d):\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    a = c - b ** 2 / 3\n",
    "    d = 2/27*b**3 - b*c/3 + d\n",
    "    c = (a/3)**3 + (d/2)**2\n",
    "    c = torch.sqrt(c)\n",
    "    alpha = -d/2 + c\n",
    "    beta = -d/2 - c\n",
    "    alpha = complex_cbrt(alpha)\n",
    "    beta = complex_cbrt(beta)\n",
    "    \n",
    "    lr1 = torch.real(alpha+beta)\n",
    "    lr2 = torch.real((-(alpha+beta) + 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    lr3 = torch.real((-(alpha+beta) - 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    return torch.cat([lr1, lr2, lr3, torch.zeros_like(lr1)],dim=-1)\n",
    "\n",
    "\n",
    "def quadratic_solver(c,d,e):\n",
    "    D = torch.sqrt(d ** 2 - 4 * e * c)\n",
    "    x1 = torch.real((-d + D)/(2*c))\n",
    "    x2 = torch.real((-d - D)/(2*c))\n",
    "    return torch.cat([x1, x2, torch.zeros_like(x1), torch.zeros_like(x1)], dim=-1)\n",
    "\n",
    "def polynomial_solver(a,b,c,d,e):\n",
    "    return torch.where(a!=0, \n",
    "                       quartic_solver(a,b,c,d,e), torch.where(\n",
    "                        b!=0,\n",
    "                        cubic_solver(b,c,d,e), torch.where(\n",
    "                        c!=0,\n",
    "                        quadratic_solver(c,d,e), torch.where(\n",
    "                        d!=0, \n",
    "                        (e/d).repeat((len(a.shape)-1)*(1,)+(4,)), \n",
    "                        e.repeat((len(a.shape)-1)*(1,)+(4,))))))\n",
    "\n",
    "\n",
    "def optimal_lr(A, x):\n",
    "    \"\"\"Calculate optimal learning rate based on matrix A and vector x.\"\"\"\n",
    "    # Compute intermediate vectors\n",
    "    u = A(x)\n",
    "    v = A(u)\n",
    "    w = A(v)\n",
    "    \n",
    "    # Compute inner products\n",
    "    a_0 = torch.sum(x*x, dim = -1, keepdim = True)\n",
    "    a_1 = torch.sum(u*x, dim = -1, keepdim = True)\n",
    "    a_2 = torch.sum(v*x, dim = -1, keepdim = True)\n",
    "    a_3 = torch.sum(w*x, dim = -1, keepdim = True)\n",
    "    a_4 = torch.sum(w*u, dim = -1, keepdim = True)\n",
    "    a_5 = torch.sum(w*v, dim = -1, keepdim = True)\n",
    "    a_6 = torch.sum(w*w, dim = -1, keepdim = True)\n",
    "\n",
    "    # Calculate r_0, r_1, r_2\n",
    "    r_0 = 4 * a_2 / (a_1 * a_1) - 2 * a_1 / (a_1 * a_0) - 2 * a_3 / (a_1 * a_2) - 2 * a_3 / (a_1 * a_2) + a_4 / (a_2 * a_2) + a_2 / (a_0 * a_2) - 2 * a_1 / (a_1 * a_0) + a_0 / (a_0 * a_0) + a_2 / (a_0 * a_2)\n",
    "    r_1 = 4 * a_3 / (a_1 * a_1) - 2 * a_2 / (a_1 * a_0) - 2 * a_4 / (a_1 * a_2) - 2 * a_4 / (a_1 * a_2) + a_5 / (a_2 * a_2) + a_3 / (a_0 * a_2) - 2 * a_2 / (a_1 * a_0) + a_1 / (a_0 * a_0) + a_3 / (a_0 * a_2)\n",
    "    r_2 = 4 * a_4 / (a_1 * a_1) - 2 * a_3 / (a_1 * a_0) - 2 * a_5 / (a_1 * a_2) - 2 * a_5 / (a_1 * a_2) + a_6 / (a_2 * a_2) + a_4 / (a_0 * a_2) - 2 * a_3 / (a_1 * a_0) + a_2 / (a_0 * a_0) + a_4 / (a_0 * a_2)\n",
    "\n",
    "    # Calculate q and p\n",
    "    q_1 = 2 * a_2 / a_1 - a_1 / a_0 - a_3 / a_2\n",
    "    q_2 = 2 * a_3 / a_1 - a_2 / a_0 - a_4 / a_2\n",
    "    #p_0 = a_0\n",
    "    #p_1 = a_1\n",
    "    #p_2 = a_2\n",
    "\n",
    "    a = r_0 * r_1 * q_2 - 2 * r_0 * q_1 * r_2\n",
    "    b = a_0 * r_1 * r_2 - 2 * a_1 * r_0 * r_2 + a_2 * r_0 * r_1 - 2 * q_1 * q_2 * r_0\n",
    "    c = 3 * a_0 * r_1 * q_2 - 3 * r_0 * a_1 * q_2\n",
    "    d = 2 * a_0 * q_1 * q_2 + 2 * a_0 * r_1 * a_2 - a_0 * a_1 * r_2 - r_0 * a_1 * a_2\n",
    "    e = 2 * a_0 * q_1 * a_2 - a_0 * a_1 * q_2\n",
    "    lr = polynomial_solver(a,b,c,d,e)\n",
    "    g = a_1 + 2 * lr * q_1 + lr ** 2 * r_1\n",
    "    f = a_0 + lr ** 2 * r_0\n",
    "    h = a_2 + 2 * lr * q_2 + lr ** 2 * r_2\n",
    "    eigenness = g**2/(f*h)\n",
    "    n = torch.argmax(eigenness, dim=-1, keepdim=True)\n",
    "    lr = torch.gather(lr, -1, n)\n",
    "    return torch.gather(eigenness, -1, n), lr * (-x / a_0 + 2 * u / a_1 - v / a_2), n, lr\n",
    "    return torch.gather(eigenness, -1, n), (-lr / a_0) * x + (2 * lr / a_1) * u + (-lr / a_2) * v, n, lr\n",
    "\n",
    "\n",
    "def update_vector(x, v):\n",
    "    x = x + v\n",
    "    return x/torch.linalg.norm(x, dim = -1, keepdim = True)\n",
    "\n",
    "def grad_ascend_lr(A,x,threshold,steps_already,steps_max):\n",
    "    e,v,i,l = optimal_lr(A, x)\n",
    "    x = update_vector(x, v)\n",
    "    steps = steps_already + 1\n",
    "    cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    while cond.any():\n",
    "        steps = torch.where(cond, steps + 1, steps)\n",
    "        e,v,i,l = optimal_lr(A, x)\n",
    "        x = torch.where(cond, update_vector(x, v), x)\n",
    "        cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    v=[]\n",
    "    f=A(x)\n",
    "    #print_gpu_memory()\n",
    "    return x, torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1).unsqueeze(-1)**2, (\n",
    "            f-(torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1)).unsqueeze(-1)**2*x\n",
    "            )/torch.linalg.norm(f, dim=-1).unsqueeze(-1), steps\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "class square(nn.Module):\n",
    "    def __init__(self, g):\n",
    "        super(square, self).__init__()\n",
    "        self.g=g\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.g(self.g(x))\n",
    "\n",
    "class minus_f(nn.Module):\n",
    "    def __init__(self, g, f):\n",
    "        super(minus_f, self).__init__()\n",
    "        self.g=g\n",
    "        self.f=f\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.g(x) - self.f*x\n",
    "\n",
    "class t_minus(nn.Module):\n",
    "    def __init__(self, g, t):\n",
    "        super(t_minus, self).__init__()\n",
    "        self.g=g\n",
    "        self.t=t\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.t*x - self.g(x)\n",
    "\n",
    "\n",
    "class modified(nn.Module):\n",
    "    def __init__(self,expr,A,batch_size):\n",
    "        super(modified, self).__init__()\n",
    "        self.parts=[]\n",
    "        for arg in expr.args:\n",
    "            self.parts.append(modified(arg,A,batch_size))\n",
    "        if expr.is_Number:\n",
    "            self.param = nn.Parameter(torch.random.uniform((batch_size,1))*float(expr))\n",
    "        self.f=torch.rand(batch_size,1).cuda()\n",
    "        self.t=((torch.rand(batch_size,1).cuda())*torch.max(torch.cat([self.f**2, (1-self.f)**2], dim=1), dim=1, keepdim = True)[0]).cuda()\n",
    "        self.z=torch.max(torch.cat([self.t**2, (self.t-self.f**2)**2, (self.t-(1-self.f)**2)**2], dim=1), dim=1, keepdim = True)[0]\n",
    "        if expr == sympy.Symbol('t'):\n",
    "            self.A=A\n",
    "        self.expr=expr\n",
    "        self.minus_f = minus_f(self.A, self.f)\n",
    "        self.square_1 = square(self.minus_f)\n",
    "        self.t_minus = t_minus(self.square_1, self.t)\n",
    "        self.square_2 = square(self.t_minus)\n",
    "    \n",
    "    def forward_1(self,x):\n",
    "        return x - self.square_2(x)/self.z\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.forward_1(x)\n",
    "        y = self.forward_1(y)\n",
    "        y = self.forward_1(y)\n",
    "        return self.forward_1(y) \n",
    "        if self.expr == sympy.Symbol('t'):\n",
    "            return self.A(x)\n",
    "\n",
    "        if self.expr.is_Number:\n",
    "            return self.param*x\n",
    "\n",
    "        if self.expr.is_Add:\n",
    "            # Initialize res as float zeros to avoid dtype conflicts\n",
    "            res = torch.zeros_like(x)\n",
    "            for arg in self.parts:\n",
    "                res += arg(x)\n",
    "            return res\n",
    "\n",
    "        if self.expr.is_Mul:\n",
    "            # product: to mimic polynomial multiplication\n",
    "            for arg in self.parts:\n",
    "                x = arg(x)\n",
    "            return x\n",
    "\n",
    "        if self.expr.is_Pow:\n",
    "            for _ in range(self.parts[1]):\n",
    "                x = self.parts[0](x)\n",
    "            return x\n",
    " \n",
    "\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "batch_size=16384\n",
    "matrix_size=256\n",
    "A=torch.diag(1/(1+torch.tensor(range(matrix_size), dtype=torch.float64)))\n",
    "\n",
    "import scipy\n",
    "\n",
    "C = torch.tensor(scipy.stats.ortho_group.rvs(dim=matrix_size), dtype=torch.float64)\n",
    "A = torch.linalg.inv(C) @ A @ C\n",
    "\n",
    "f=nn.Linear(matrix_size,matrix_size, bias=False).cuda()\n",
    "f.weight=nn.Parameter(A.cuda(), requires_grad=False)\n",
    "#f.bias=nn.Parameter(torch.zeros_like(f.bias))\n",
    "\n",
    "h = nn.Linear(matrix_size, matrix_size, bias=False).cuda()\n",
    "h.weight=nn.Parameter(C.cuda(), requires_grad=False)\n",
    "\n",
    "probs = 0.5*torch.ones((batch_size, 1)).cuda()\n",
    "delta = torch.tensor(1e-6).cuda()\n",
    "\n",
    "class truncated(nn.Module):\n",
    "    def __init__(self, f, x):\n",
    "        super(truncated, self).__init__()\n",
    "        \n",
    "        y = f(x)\n",
    "        xx = torch.sum(x**2, dim = -1, keepdim = True)\n",
    "        xAx = torch.sum(x*y, dim = -1, keepdim = True)\n",
    "        xAAx = torch.sum(y**2, dim = -1, keepdim = True)\n",
    "        mean = xAx/xx\n",
    "        sigma = torch.sqrt(xAAx*xx - xAx**2)/xx\n",
    "        self.mean = mean + (-1)**torch.bernoulli(probs) * sigma\n",
    "        self.sigma = torch.sqrt(-2*sigma*torch.log(delta))\n",
    "        self.lambda_min = self.mean - self.sigma\n",
    "        self.lambda_max = self.mean + self.sigma\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.f(x) - self.mean * x\n",
    "        y = self.f(y) - self.mean * y\n",
    "        return x - y / torch.min(torch.cat((\n",
    "            (self.lambda_min - self.mean)**2, \n",
    "            (self.lambda_max - self.mean)**2), \n",
    "                dim = -1), dim = -1, keepdim = True)[0]\n",
    "\n",
    "result=np.zeros((matrix_size,))\n",
    "expr_1=t\n",
    "print(expr_1)\n",
    "t=[1]\n",
    "stepses=[]\n",
    "with torch.cuda.device(0):\n",
    "    for i in range(1):\n",
    "        x = torch.empty((batch_size, matrix_size), dtype=torch.float64).normal_(mean = 0, std = 1).cuda()\n",
    "        steps = torch.zeros((batch_size, 1)).cuda()\n",
    "        stepses.append(torch.zeros((batch_size, 1)).cuda())\n",
    "        t[0] = x/torch.linalg.norm(x, dim = -1, keepdim = True)\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "        g = modified(expr_1, f, batch_size)\n",
    "        s = grad_ascend_lr(g, t[0], 1e-30, steps, 2000)\n",
    "        t[0] = torch.where(cond, s[0], t[0])\n",
    "        steps = s[-1]\n",
    "        stepses[i] = torch.where(cond, s[-1], stepses[i])\n",
    "        s = [1]\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "        g = truncated(f, t[0])\n",
    "        while torch.any((steps < 2000) & cond):\n",
    "            s = grad_ascend_lr(g, t[0], 1e-30, steps, 2000)\n",
    "            t[0] = torch.where(cond, s[0], t[0])\n",
    "            steps = s[-1]\n",
    "            stepses[i] = torch.where(cond, s[-1], stepses[i])\n",
    "            s = [1]\n",
    "            cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "            g = truncated(f, t[0])\n",
    "        result+= torch.sum(torch.where(torch.abs(h(t[0])) >= 0.9, 1, 0), dim = 0).cpu().numpy()\n",
    "        print(i, list(np.where(result > 0)[0]), np.sum(result) / ((i + 1) * batch_size), end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2faad37c-eddd-4d13-9ead-d8fb73f38616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10097.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c5f00e5-3b5b-49b9-b8d7-eb363d94f706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14014/1674084335.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 5.68 GiB of which 1.62 GiB is free. Process 6834 has 1.78 GiB memory in use. Including non-PyTorch memory, this process has 2.25 GiB memory in use. Of the allocated memory 602.42 MiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 346\u001b[0m\n\u001b[1;32m    344\u001b[0m cond \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mabs(h(t[\u001b[38;5;241m0\u001b[39m])), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m    345\u001b[0m g \u001b[38;5;241m=\u001b[39m modified(expr_1, f, batch_size)\n\u001b[0;32m--> 346\u001b[0m s \u001b[38;5;241m=\u001b[39m grad_ascend_lr(g, t[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1e-30\u001b[39m, steps, \u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m    347\u001b[0m t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(cond, s[\u001b[38;5;241m0\u001b[39m], t[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    348\u001b[0m steps \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[47], line 194\u001b[0m, in \u001b[0;36mgrad_ascend_lr\u001b[0;34m(A, x, threshold, steps_already, steps_max)\u001b[0m\n\u001b[1;32m    192\u001b[0m f\u001b[38;5;241m=\u001b[39mA(x)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m#print_gpu_memory()\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mij,ij->i\u001b[39m\u001b[38;5;124m'\u001b[39m,f,x)\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, (\n\u001b[1;32m    195\u001b[0m         f\u001b[38;5;241m-\u001b[39m(torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mij,ij->i\u001b[39m\u001b[38;5;124m'\u001b[39m,f,x)\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mx\n\u001b[1;32m    196\u001b[0m         )\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(f, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), steps\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 5.68 GiB of which 1.62 GiB is free. Process 6834 has 1.78 GiB memory in use. Including non-PyTorch memory, this process has 2.25 GiB memory in use. Of the allocated memory 602.42 MiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sympy\n",
    "from sympy import Symbol, Mul, Pow, Add\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Prints the current allocated, reserved, and free memory on the GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "        free_memory = torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device)  # Free memory in reserved area\n",
    "        free_memory_mb = free_memory / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "        print(f\"Allocated memory: {allocated_memory:.2f} MB\")\n",
    "        print(f\"Reserved memory: {reserved_memory:.2f} MB\")\n",
    "        print(f\"Free memory: {free_memory_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "t=sympy.Symbol('t')\n",
    "sqrt_3=torch.sqrt(torch.tensor([3.])).cuda()\n",
    "\n",
    "\n",
    "def complex_cbrt(z):\n",
    "    \"\"\"Calculate the cubic root of a complex number.\"\"\"\n",
    "    # Calculate the magnitude and angle of the complex number\n",
    "    r = z.abs()  # Magnitude\n",
    "    theta = torch.angle(z)  # Angle in radians\n",
    "    #print(\"z\")\n",
    "    #print(z)\n",
    "    #print(\"r\")\n",
    "    #print(r)\n",
    "    #print(\"theta\")\n",
    "    #print(theta)\n",
    "\n",
    "    # Calculate the cubic root of the magnitude and the angle divided by 3\n",
    "    r = r ** (1/3)  # Magnitude of the cubic root\n",
    "    theta = theta / 3  # Angle of the cubic root\n",
    "    #print(\"root_magnitude\")\n",
    "    #print(r)\n",
    "    #print(\"root_angle\")\n",
    "    #print(theta)\n",
    "\n",
    "    return r * torch.cos(theta) + 1j * r * torch.sin(theta)  # Return as a complex number\n",
    "\n",
    "def quartic_solver(a,b,c,d,e):\n",
    "    e = e/a\n",
    "    d = d/a\n",
    "    c = c/a\n",
    "    b = b/a\n",
    "    i, lr_1 = torch.frexp(e)\n",
    "    i, lr_2 = torch.frexp(d)\n",
    "    i, lr_3 = torch.frexp(c)\n",
    "    i, lr_4 = torch.frexp(b)\n",
    "    i = torch.max(\n",
    "        torch.cat((2 + 3 * torch.max(torch.cat((2 * lr_3, 2 + lr_4 + lr_2, 4 + lr_1), \n",
    "                                           dim = -1), dim = -1, keepdim = True)[0], \n",
    "                  2 * torch.max(torch.cat((1 + 3 * lr_3, 3 + lr_4 + lr_3 + lr_2, 5 + 2 * lr_4 + lr_1, 5 + 2 * lr_2, 8 + lr_3 + lr_1), \n",
    "                                        dim = -1), dim = -1, keepdim = True)[0]), \n",
    "                  dim = -1), dim = -1, keepdim = True)[0]//12\n",
    "        \n",
    "    a = c ** 2 - 3 * b * d + 12 * e\n",
    "    e = 2 * c ** 3 - 9 * b * c * d + 27 * b ** 2 * e + 27 * d ** 2 - 72 * c * e\n",
    "    e = torch.ldexp(e, -6*i)\n",
    "    a = torch.ldexp(a, -4*i)\n",
    "    #print(\"Delta_0\")\n",
    "    #print(a)\n",
    "    #print(\"Delta_1\")\n",
    "    #print(e)\n",
    "    #print(\"under sqrt\")\n",
    "    #print(e ** 2 - 4 * a ** 3)\n",
    "    #print(\"sqrt\")\n",
    "    #print(torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64)))\n",
    "\n",
    "    Q = complex_cbrt((e + torch.sqrt(torch.tensor(e ** 2 - 4 * a ** 3, dtype=torch.complex64))) / 2) \n",
    "    #print(\"Q\")\n",
    "    #print(Q)\n",
    "    \n",
    "    # Calculate p and q\n",
    "    e = torch.ldexp(c - 0.375 * b ** 2, -2*i)\n",
    "    d = torch.ldexp((0.5*b) ** 3 - 0.5 * b * c + d, -3*i)\n",
    "    #print(\"q\")\n",
    "    #print(d)\n",
    "    S = torch.sqrt(-2 / 3 * e + (Q + a / Q) / 3) / 2\n",
    "    a = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e + d / S)\n",
    "    c = 0.5*torch.sqrt(-4 * S ** 2 - 2 * e - d / S)\n",
    "    b = torch.ldexp(-b, -i-2)\n",
    "\n",
    "    lr_1 = torch.ldexp(torch.real(b - S + a), i)\n",
    "    lr_2 = torch.ldexp(torch.real(b - S - a), i)\n",
    "    lr_3 = torch.ldexp(torch.real(b + S + c), i)\n",
    "    lr_4 = torch.ldexp(torch.real(b + S - c), i)\n",
    "    \n",
    "    return torch.cat([lr_1, lr_2, lr_3, lr_4],dim = -1)\n",
    "\n",
    "def cubic_solver(a,b,c,d):\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    a = c - b ** 2 / 3\n",
    "    d = 2/27*b**3 - b*c/3 + d\n",
    "    c = (a/3)**3 + (d/2)**2\n",
    "    c = torch.sqrt(c)\n",
    "    alpha = -d/2 + c\n",
    "    beta = -d/2 - c\n",
    "    alpha = complex_cbrt(alpha)\n",
    "    beta = complex_cbrt(beta)\n",
    "    \n",
    "    lr1 = torch.real(alpha+beta)\n",
    "    lr2 = torch.real((-(alpha+beta) + 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    lr3 = torch.real((-(alpha+beta) - 1j * sqrt_3 * (alpha-beta))/2)\n",
    "    return torch.cat([lr1, lr2, lr3, torch.zeros_like(lr1)],dim=-1)\n",
    "\n",
    "\n",
    "def quadratic_solver(c,d,e):\n",
    "    D = torch.sqrt(d ** 2 - 4 * e * c)\n",
    "    x1 = torch.real((-d + D)/(2*c))\n",
    "    x2 = torch.real((-d - D)/(2*c))\n",
    "    return torch.cat([x1, x2, torch.zeros_like(x1), torch.zeros_like(x1)], dim=-1)\n",
    "\n",
    "def polynomial_solver(a,b,c,d,e):\n",
    "    return torch.where(a!=0, \n",
    "                       quartic_solver(a,b,c,d,e), torch.where(\n",
    "                        b!=0,\n",
    "                        cubic_solver(b,c,d,e), torch.where(\n",
    "                        c!=0,\n",
    "                        quadratic_solver(c,d,e), torch.where(\n",
    "                        d!=0, \n",
    "                        (e/d).repeat((len(a.shape)-1)*(1,)+(4,)), \n",
    "                        e.repeat((len(a.shape)-1)*(1,)+(4,))))))\n",
    "\n",
    "\n",
    "def optimal_lr(A, x):\n",
    "    \"\"\"Calculate optimal learning rate based on matrix A and vector x.\"\"\"\n",
    "    # Compute intermediate vectors\n",
    "    u = A(x)\n",
    "    v = A(u)\n",
    "    w = A(v)\n",
    "    \n",
    "    # Compute inner products\n",
    "    a_0 = torch.sum(x*x, dim = -1, keepdim = True)\n",
    "    a_1 = torch.sum(u*x, dim = -1, keepdim = True)\n",
    "    a_2 = torch.sum(v*x, dim = -1, keepdim = True)\n",
    "    a_3 = torch.sum(w*x, dim = -1, keepdim = True)\n",
    "    a_4 = torch.sum(w*u, dim = -1, keepdim = True)\n",
    "    a_5 = torch.sum(w*v, dim = -1, keepdim = True)\n",
    "    a_6 = torch.sum(w*w, dim = -1, keepdim = True)\n",
    "\n",
    "    # Calculate r_0, r_1, r_2\n",
    "    r_0 = 4 * a_2 / (a_1 * a_1) - 2 * a_1 / (a_1 * a_0) - 2 * a_3 / (a_1 * a_2) - 2 * a_3 / (a_1 * a_2) + a_4 / (a_2 * a_2) + a_2 / (a_0 * a_2) - 2 * a_1 / (a_1 * a_0) + a_0 / (a_0 * a_0) + a_2 / (a_0 * a_2)\n",
    "    r_1 = 4 * a_3 / (a_1 * a_1) - 2 * a_2 / (a_1 * a_0) - 2 * a_4 / (a_1 * a_2) - 2 * a_4 / (a_1 * a_2) + a_5 / (a_2 * a_2) + a_3 / (a_0 * a_2) - 2 * a_2 / (a_1 * a_0) + a_1 / (a_0 * a_0) + a_3 / (a_0 * a_2)\n",
    "    r_2 = 4 * a_4 / (a_1 * a_1) - 2 * a_3 / (a_1 * a_0) - 2 * a_5 / (a_1 * a_2) - 2 * a_5 / (a_1 * a_2) + a_6 / (a_2 * a_2) + a_4 / (a_0 * a_2) - 2 * a_3 / (a_1 * a_0) + a_2 / (a_0 * a_0) + a_4 / (a_0 * a_2)\n",
    "\n",
    "    # Calculate q and p\n",
    "    q_1 = 2 * a_2 / a_1 - a_1 / a_0 - a_3 / a_2\n",
    "    q_2 = 2 * a_3 / a_1 - a_2 / a_0 - a_4 / a_2\n",
    "    #p_0 = a_0\n",
    "    #p_1 = a_1\n",
    "    #p_2 = a_2\n",
    "\n",
    "    a = r_0 * r_1 * q_2 - 2 * r_0 * q_1 * r_2\n",
    "    b = a_0 * r_1 * r_2 - 2 * a_1 * r_0 * r_2 + a_2 * r_0 * r_1 - 2 * q_1 * q_2 * r_0\n",
    "    c = 3 * a_0 * r_1 * q_2 - 3 * r_0 * a_1 * q_2\n",
    "    d = 2 * a_0 * q_1 * q_2 + 2 * a_0 * r_1 * a_2 - a_0 * a_1 * r_2 - r_0 * a_1 * a_2\n",
    "    e = 2 * a_0 * q_1 * a_2 - a_0 * a_1 * q_2\n",
    "    lr = polynomial_solver(a,b,c,d,e)\n",
    "    g = a_1 + 2 * lr * q_1 + lr ** 2 * r_1\n",
    "    f = a_0 + lr ** 2 * r_0\n",
    "    h = a_2 + 2 * lr * q_2 + lr ** 2 * r_2\n",
    "    eigenness = g**2/(f*h)\n",
    "    n = torch.argmax(eigenness, dim=-1, keepdim=True)\n",
    "    lr = torch.gather(lr, -1, n)\n",
    "    return torch.gather(eigenness, -1, n), lr * (-x / a_0 + 2 * u / a_1 - v / a_2), n, lr\n",
    "    return torch.gather(eigenness, -1, n), (-lr / a_0) * x + (2 * lr / a_1) * u + (-lr / a_2) * v, n, lr\n",
    "\n",
    "\n",
    "def update_vector(x, v):\n",
    "    x = x + v\n",
    "    return x/torch.linalg.norm(x, dim = -1, keepdim = True)\n",
    "\n",
    "def grad_ascend_lr(A,x,threshold,steps_already,steps_max):\n",
    "    e,v,i,l = optimal_lr(A, x)\n",
    "    x = update_vector(x, v)\n",
    "    steps = steps_already + 1\n",
    "    cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    while cond.any():\n",
    "        steps = torch.where(cond, steps + 1, steps)\n",
    "        e,v,i,l = optimal_lr(A, x)\n",
    "        x = torch.where(cond, update_vector(x, v), x)\n",
    "        cond = (1 - e > threshold) & (steps < steps_max)\n",
    "    v=[]\n",
    "    f=A(x)\n",
    "    #print_gpu_memory()\n",
    "    return x, torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1).unsqueeze(-1)**2, (\n",
    "            f-(torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1)).unsqueeze(-1)**2*x\n",
    "            )/torch.linalg.norm(f, dim=-1).unsqueeze(-1), steps\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "class square(nn.Module):\n",
    "    def __init__(self, g):\n",
    "        super(square, self).__init__()\n",
    "        self.g=g\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.g(self.g(x))\n",
    "\n",
    "class minus_f(nn.Module):\n",
    "    def __init__(self, g, f):\n",
    "        super(minus_f, self).__init__()\n",
    "        self.g=g\n",
    "        self.f=f\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.g(x) - self.f*x\n",
    "\n",
    "class t_minus(nn.Module):\n",
    "    def __init__(self, g, t):\n",
    "        super(t_minus, self).__init__()\n",
    "        self.g=g\n",
    "        self.t=t\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.t*x - self.g(x)\n",
    "\n",
    "\n",
    "class modified(nn.Module):\n",
    "    def __init__(self,expr,A,batch_size):\n",
    "        super(modified, self).__init__()\n",
    "        self.parts=[]\n",
    "        for arg in expr.args:\n",
    "            self.parts.append(modified(arg,A,batch_size))\n",
    "        if expr.is_Number:\n",
    "            self.param = nn.Parameter(torch.random.uniform((batch_size,1))*float(expr))\n",
    "        self.f=torch.rand(batch_size,1).cuda()\n",
    "        self.t=((torch.rand(batch_size,1).cuda())*torch.max(torch.cat([self.f**2, (1-self.f)**2], dim=1), dim=1, keepdim = True)[0]).cuda()\n",
    "        self.z=torch.max(torch.cat([self.t**2, (self.t-self.f**2)**2, (self.t-(1-self.f)**2)**2], dim=1), dim=1, keepdim = True)[0]\n",
    "        if expr == sympy.Symbol('t'):\n",
    "            self.A=A\n",
    "        self.expr=expr\n",
    "        self.minus_f = minus_f(self.A, self.f)\n",
    "        self.square_1 = square(self.minus_f)\n",
    "        self.t_minus = t_minus(self.square_1, self.t)\n",
    "        self.square_2 = square(self.t_minus)\n",
    "    \n",
    "    def forward_1(self,x):\n",
    "        return x - self.square_2(x)/self.z\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.forward_1(x)\n",
    "        y = self.forward_1(y)\n",
    "        y = self.forward_1(y)\n",
    "        return self.forward_1(y) \n",
    "        if self.expr == sympy.Symbol('t'):\n",
    "            return self.A(x)\n",
    "\n",
    "        if self.expr.is_Number:\n",
    "            return self.param*x\n",
    "\n",
    "        if self.expr.is_Add:\n",
    "            # Initialize res as float zeros to avoid dtype conflicts\n",
    "            res = torch.zeros_like(x)\n",
    "            for arg in self.parts:\n",
    "                res += arg(x)\n",
    "            return res\n",
    "\n",
    "        if self.expr.is_Mul:\n",
    "            # product: to mimic polynomial multiplication\n",
    "            for arg in self.parts:\n",
    "                x = arg(x)\n",
    "            return x\n",
    "\n",
    "        if self.expr.is_Pow:\n",
    "            for _ in range(self.parts[1]):\n",
    "                x = self.parts[0](x)\n",
    "            return x\n",
    " \n",
    "\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "batch_size=16384\n",
    "matrix_size=32\n",
    "A=torch.diag(torch.flatten(1/(1+torch.tensor(range(matrix_size), dtype=torch.float64)).unsqueeze(0)+\n",
    "                           1/(1+torch.tensor(range(matrix_size), dtype=torch.float64)).unsqueeze(1)))\n",
    "\n",
    "matrix_size=matrix_size**2\n",
    "import scipy\n",
    "\n",
    "C = torch.tensor(scipy.stats.ortho_group.rvs(dim=matrix_size), dtype=torch.float64)\n",
    "A = torch.linalg.inv(C) @ A @ C\n",
    "\n",
    "f=nn.Linear(matrix_size,matrix_size, bias=False).cuda()\n",
    "f.weight=nn.Parameter(A.cuda(), requires_grad=False)\n",
    "#f.bias=nn.Parameter(torch.zeros_like(f.bias))\n",
    "\n",
    "h = nn.Linear(matrix_size, matrix_size, bias=False).cuda()\n",
    "h.weight=nn.Parameter(C.cuda(), requires_grad=False)\n",
    "\n",
    "probs = 0.5*torch.ones((batch_size, 1)).cuda()\n",
    "delta = torch.tensor(1e-6).cuda()\n",
    "\n",
    "class truncated(nn.Module):\n",
    "    def __init__(self, f, x):\n",
    "        super(truncated, self).__init__()\n",
    "        \n",
    "        y = f(x)\n",
    "        xx = torch.sum(x**2, dim = -1, keepdim = True)\n",
    "        xAx = torch.sum(x*y, dim = -1, keepdim = True)\n",
    "        xAAx = torch.sum(y**2, dim = -1, keepdim = True)\n",
    "        mean = xAx/xx\n",
    "        sigma = torch.sqrt(xAAx*xx - xAx**2)/xx\n",
    "        self.mean = mean + (-1)**torch.bernoulli(probs) * sigma\n",
    "        self.sigma = torch.sqrt(-2*sigma*torch.log(delta))\n",
    "        self.lambda_min = self.mean - self.sigma\n",
    "        self.lambda_max = self.mean + self.sigma\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.f(x) - self.mean * x\n",
    "        y = self.f(y) - self.mean * y\n",
    "        return x - y / torch.min(torch.cat((\n",
    "            (self.lambda_min - self.mean)**2, \n",
    "            (self.lambda_max - self.mean)**2), \n",
    "                dim = -1), dim = -1, keepdim = True)[0]\n",
    "\n",
    "result=np.zeros((matrix_size,))\n",
    "expr_1=t\n",
    "print(expr_1)\n",
    "t=[1]\n",
    "stepses=[]\n",
    "with torch.cuda.device(0):\n",
    "    for i in range(1):\n",
    "        x = torch.empty((batch_size, matrix_size), dtype=torch.float64).normal_(mean = 0, std = 1).cuda()\n",
    "        steps = torch.zeros((batch_size, 1)).cuda()\n",
    "        stepses.append(torch.zeros((batch_size, 1)).cuda())\n",
    "        t[0] = x/torch.linalg.norm(x, dim = -1, keepdim = True)\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "        g = modified(expr_1, f, batch_size)\n",
    "        s = grad_ascend_lr(g, t[0], 1e-30, steps, 2000)\n",
    "        t[0] = torch.where(cond, s[0], t[0])\n",
    "        steps = s[-1]\n",
    "        stepses[i] = torch.where(cond, s[-1], stepses[i])\n",
    "        s = [1]\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "        g = truncated(f, t[0])\n",
    "        while torch.any((steps < 2000) & cond):\n",
    "            s = grad_ascend_lr(g, t[0], 1e-30, steps, 2000)\n",
    "            t[0] = torch.where(cond, s[0], t[0])\n",
    "            steps = s[-1]\n",
    "            stepses[i] = torch.where(cond, s[-1], stepses[i])\n",
    "            s = [1]\n",
    "            cond = (torch.max(torch.abs(h(t[0])), dim = -1, keepdim = True)[0] < 0.9)\n",
    "            g = truncated(f, t[0])\n",
    "        result+= torch.sum(torch.where(torch.abs(h(t[0])) >= 0.9, 1, 0), dim = 0).cpu().numpy()\n",
    "        print(i, list(np.where(result > 0)[0]), np.sum(result) / ((i + 1) * batch_size), end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab800d-4212-4322-8c7e-dec487fe0732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
