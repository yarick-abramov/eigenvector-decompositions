{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39086db-7e80-4769-b71d-ed06c060ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device=torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92bb6517-9853-4509-bf72-175fe07149b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102297/602145872.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Q = complex_cbrt((Delta_1 + torch.sqrt(torch.tensor(Delta_1 ** 2 - 4 * Delta_0 ** 3, dtype=torch.complex64))) / 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 29, 35]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 252\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many((steps\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m2000\u001b[39m) \u001b[38;5;241m&\u001b[39m cond):\n\u001b[1;32m    251\u001b[0m     g \u001b[38;5;241m=\u001b[39m modified(expr_1,f,batch_size)\n\u001b[0;32m--> 252\u001b[0m     s \u001b[38;5;241m=\u001b[39m grad_ascend_lr(g,t[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m0.9999\u001b[39m,steps,\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m    253\u001b[0m     t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(cond, s[\u001b[38;5;241m0\u001b[39m], t[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    254\u001b[0m     steps \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[13], line 154\u001b[0m, in \u001b[0;36mgrad_ascend_lr\u001b[0;34m(A, x, threshold, steps_already, steps_max)\u001b[0m\n\u001b[1;32m    152\u001b[0m steps \u001b[38;5;241m=\u001b[39m steps_already\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    153\u001b[0m cond \u001b[38;5;241m=\u001b[39m (e\u001b[38;5;241m<\u001b[39mthreshold) \u001b[38;5;241m&\u001b[39m (steps\u001b[38;5;241m<\u001b[39msteps_max)\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    155\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(cond, update_vector(x, v), x)\n\u001b[1;32m    156\u001b[0m     steps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(cond, steps\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, steps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sympy\n",
    "from sympy import Symbol, Mul, Pow, Add\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Prints the current allocated, reserved, and free memory on the GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "        free_memory = torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device)  # Free memory in reserved area\n",
    "        free_memory_mb = free_memory / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "        print(f\"Allocated memory: {allocated_memory:.2f} MB\")\n",
    "        print(f\"Reserved memory: {reserved_memory:.2f} MB\")\n",
    "        print(f\"Free memory: {free_memory_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "t=sympy.Symbol('t')\n",
    "sqrt_3=torch.sqrt(torch.tensor([3.])).cuda()\n",
    "\n",
    "\n",
    "def complex_cbrt(z):\n",
    "    \"\"\"Calculate the cubic root of a complex number.\"\"\"\n",
    "    # Calculate the magnitude and angle of the complex number\n",
    "    r = z.abs()  # Magnitude\n",
    "    theta = torch.angle(z)  # Angle in radians\n",
    "\n",
    "    # Calculate the cubic root of the magnitude and the angle divided by 3\n",
    "    root_magnitude = r ** (1/3)  # Magnitude of the cubic root\n",
    "    root_angle = theta / 3  # Angle of the cubic root\n",
    "\n",
    "    # Convert back to rectangular form (Cartesian coordinates)\n",
    "    root_real = root_magnitude * torch.cos(root_angle)\n",
    "    root_imag = root_magnitude * torch.sin(root_angle)\n",
    "\n",
    "    return root_real + 1j * root_imag  # Return as a complex number\n",
    "\n",
    "def quartic_solver(a,b,c,d,e):\n",
    "    e=e/a\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    Delta_0 = c ** 2 - 3 * b * d + 12 * e\n",
    "    Delta_1 = 2 * c ** 3 - 9 * b * c * d + 27 * b ** 2 * e + 27 * d ** 2 - 72 * c * e\n",
    "\n",
    "    Q = complex_cbrt((Delta_1 + torch.sqrt(torch.tensor(Delta_1 ** 2 - 4 * Delta_0 ** 3, dtype=torch.complex64))) / 2)\n",
    "    \n",
    "    # Calculate p and q\n",
    "    p = c - 0.375 * b ** 2\n",
    "    q = (0.5*b) ** 3 - 0.5 * b * c + d    \n",
    "    S = torch.sqrt(-2 / 3 * p + (Q + Delta_0 / Q) / 3) / 2\n",
    "    S_1 = 0.5*torch.sqrt(-4 * S ** 2 - 2 * p + q / S)\n",
    "    S_2 = 0.5*torch.sqrt(-4 * S ** 2 - 2 * p - q / S)\n",
    "    b=-0.25*b\n",
    "\n",
    "    lr_1 = torch.real(b - S + S_1)\n",
    "    lr_2 = torch.real(b - S - S_1)\n",
    "    lr_3 = torch.real(b + S + S_2)\n",
    "    lr_4 = torch.real(b + S - S_2)\n",
    "    \n",
    "    return torch.cat([lr_1,lr_2,lr_3,lr_4],dim=-1)\n",
    "\n",
    "def cubic_solver(a,b,c,d):\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    p = c - b ** 2 / 3\n",
    "    q = 2/27*b**3 - b*c/3 + d\n",
    "    Q = (p/3)**3 + (q/2)**2\n",
    "    S = torch.sqrt(Q)\n",
    "    alpha = -q/2 + S\n",
    "    beta = -q/2 - S\n",
    "    alpha = complex_cbrt(alpha)\n",
    "    beta = complex_cbrt(beta)\n",
    "    \n",
    "    lr1 = torch.real(alpha+beta)\n",
    "    lr2 = torch.real((-(alpha+beta)+1j*sqrt_3*(alpha-beta))/2)\n",
    "    lr3 = torch.real((-(alpha+beta)-1j*sqrt_3*(alpha-beta))/2)\n",
    "    return torch.cat([lr1,lr2,lr3, torch.zeros_like(lr1)],dim=-1)\n",
    "\n",
    "\n",
    "def quadratic_solver(c,d,e):\n",
    "    D = torch.sqrt(d ** 2 - 4 * e * c)\n",
    "    x1 = torch.real((-d + D)/(2*c))\n",
    "    x2 = torch.real((-d - D)/(2*c))\n",
    "    return torch.cat([x1,x2,torch.zeros_like(x1),torch.zeros_like(x1)], dim=-1)\n",
    "\n",
    "def polynomial_solver(a,b,c,d,e):\n",
    "    return torch.where(a!=0, \n",
    "                       quartic_solver(a,b,c,d,e), torch.where(\n",
    "                        b!=0,\n",
    "                        cubic_solver(b,c,d,e), torch.where(\n",
    "                        c!=0,\n",
    "                        quadratic_solver(c,d,e), torch.where(\n",
    "                        d!=0, \n",
    "                        (e/d).repeat((len(a.shape)-1)*(1,)+(4,)), \n",
    "                        e.repeat((len(a.shape)-1)*(1,)+(4,))))))\n",
    "\n",
    "\n",
    "def optimal_lr(A, x):\n",
    "    \"\"\"Calculate optimal learning rate based on matrix A and vector x.\"\"\"\n",
    "    # Compute intermediate vectors\n",
    "    u = A(x)\n",
    "    v = A(u)\n",
    "    w = A(v)\n",
    "    \n",
    "    # Compute inner products\n",
    "    a_0 = torch.einsum('ij,ij->i', x, x).unsqueeze(-1)\n",
    "    a_1 = torch.einsum('ij,ij->i', u, x).unsqueeze(-1)\n",
    "    a_2 = torch.einsum('ij,ij->i', v, x).unsqueeze(-1)\n",
    "    a_3 = torch.einsum('ij,ij->i', w, x).unsqueeze(-1)\n",
    "    a_4 = torch.einsum('ij,ij->i', w, u).unsqueeze(-1)\n",
    "    a_5 = torch.einsum('ij,ij->i', w, v).unsqueeze(-1)\n",
    "    a_6 = torch.einsum('ij,ij->i', w, w).unsqueeze(-1)\n",
    "\n",
    "    # Calculate r_0, r_1, r_2\n",
    "    r_0 = 4 * a_2 / (a_1 * a_1) - 2 * a_1 / (a_1 * a_0) - 2 * a_3 / (a_1 * a_2) - 2 * a_3 / (a_1 * a_2) + a_4 / (a_2 * a_2) + a_2 / (a_0 * a_2) - 2 * a_1 / (a_1 * a_0) + a_0 / (a_0 * a_0) + a_2 / (a_0 * a_2)\n",
    "    r_1 = 4 * a_3 / (a_1 * a_1) - 2 * a_2 / (a_1 * a_0) - 2 * a_4 / (a_1 * a_2) - 2 * a_4 / (a_1 * a_2) + a_5 / (a_2 * a_2) + a_3 / (a_0 * a_2) - 2 * a_2 / (a_1 * a_0) + a_1 / (a_0 * a_0) + a_3 / (a_0 * a_2)\n",
    "    r_2 = 4 * a_4 / (a_1 * a_1) - 2 * a_3 / (a_1 * a_0) - 2 * a_5 / (a_1 * a_2) - 2 * a_5 / (a_1 * a_2) + a_6 / (a_2 * a_2) + a_4 / (a_0 * a_2) - 2 * a_3 / (a_1 * a_0) + a_2 / (a_0 * a_0) + a_4 / (a_0 * a_2)\n",
    "\n",
    "    # Calculate q and p\n",
    "    q_1 = 2 * a_2 / a_1 - a_1 / a_0 - a_3 / a_2\n",
    "    q_2 = 2 * a_3 / a_1 - a_2 / a_0 - a_4 / a_2\n",
    "    p_0 = a_0\n",
    "    p_1 = a_1\n",
    "    p_2 = a_2\n",
    "\n",
    "    a = r_0 * r_1 * q_2 - 2 * r_0 * q_1 * r_2\n",
    "    b = p_0 * r_1 * r_2 - 2 * p_1 * r_0 * r_2 + p_2 * r_0 * r_1 - 2 * q_1 * q_2 * r_0\n",
    "    c = 3 * p_0 * r_1 * q_2 - 3 * r_0 * p_1 * q_2\n",
    "    d = 2 * p_0 * q_1 * q_2 + 2 * p_0 * r_1 * p_2 - p_0 * p_1 * r_2 - r_0 * p_1 * p_2\n",
    "    e = 2 * p_0 * q_1 * p_2 - p_0 * p_1 * q_2\n",
    "    lr = polynomial_solver(a,b,c,d,e)\n",
    "    g = a_1 + 2 * lr * q_1 + lr ** 2 * r_1\n",
    "    f = a_0 + lr ** 2 * r_0\n",
    "    h = a_2 + 2 * lr * q_2 + lr ** 2 * r_2\n",
    "    eigenness = g**2/(f*h)\n",
    "    n = torch.argmax(eigenness, dim=-1, keepdim=True)\n",
    "    lr = torch.gather(lr, -1, n)\n",
    "    return torch.gather(eigenness, -1, n), (-lr / a_0) * x + (2 * lr / a_1) * u + (-lr / a_2) * v, n, lr\n",
    "\n",
    "\n",
    "def update_vector(x, v):\n",
    "    x=x+v\n",
    "    return x/torch.linalg.norm(x, dim=-1).unsqueeze(-1)\n",
    "\n",
    "def grad_ascend_lr(A,x,threshold,steps_already,steps_max):\n",
    "    e,v,i,l = optimal_lr(A,x)\n",
    "    x = update_vector(x, v)\n",
    "    steps = steps_already+1\n",
    "    cond = (e<threshold) & (steps<steps_max)\n",
    "    while cond.any():\n",
    "        x = torch.where(cond, update_vector(x, v), x)\n",
    "        steps = torch.where(cond, steps+1, steps)\n",
    "        e,v,i,l=optimal_lr(A,x)\n",
    "        cond=(e<threshold)&(steps<steps_max)\n",
    "    f=A(x)\n",
    "    return x, torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1).unsqueeze(-1)**2, (f-(torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1)).unsqueeze(-1)**2*x)/torch.linalg.norm(f, dim=-1).unsqueeze(-1),steps\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "class modified(nn.Module):\n",
    "    def __init__(self,expr,A,batch_size):\n",
    "        super(modified, self).__init__()\n",
    "        self.parts=[]\n",
    "        for arg in expr.args:\n",
    "            self.parts.append(modified(arg,A,batch_size))\n",
    "        if expr.is_Number:\n",
    "            self.param = nn.Parameter(torch.random.uniform((batch_size,1))*float(expr))\n",
    "        self.f=torch.rand(batch_size,1).cuda()\n",
    "        self.t=(torch.rand(batch_size,1).cuda()*torch.max(torch.cat([self.f**2,(1-self.f)**2],dim=1),dim=1)[0].unsqueeze(-1)).cuda()\n",
    "        self.z=torch.max(torch.cat([self.t**2,(self.t-self.f**2)**2,(self.t-(1-self.f)**2)**2], dim=1), dim=1)[0].unsqueeze(-1)\n",
    "        if expr == sympy.Symbol('t'):\n",
    "            self.A=A\n",
    "        self.expr=expr\n",
    "    def forward_1(self,x):\n",
    "            y=self.A(x)-self.f*x\n",
    "            y=self.A(y)-self.f*y\n",
    "            y=self.t*x-y\n",
    "            z=self.A(y)-self.f*y\n",
    "            z=self.A(z)-self.f*z\n",
    "            y=self.t*y-z\n",
    "            return self.z*x-y\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        return self.forward_1(x) \n",
    "        if self.expr == sympy.Symbol('t'):\n",
    "            return self.A(x)\n",
    "\n",
    "        if self.expr.is_Number:\n",
    "            return self.param*x\n",
    "\n",
    "        if self.expr.is_Add:\n",
    "            # Initialize res as float zeros to avoid dtype conflicts\n",
    "            res = torch.zeros_like(x)\n",
    "            for arg in self.parts:\n",
    "                res += arg(x)\n",
    "            return res\n",
    "\n",
    "        if self.expr.is_Mul:\n",
    "            # product: to mimic polynomial multiplication\n",
    "            for arg in self.parts:\n",
    "                x = arg(x)\n",
    "            return x\n",
    "\n",
    "        if self.expr.is_Pow:\n",
    "            for _ in range(self.parts[1]):\n",
    "                x = self.parts[0](x)\n",
    "            return x\n",
    " \n",
    "\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "batch_size=10000\n",
    "matrix_size=1000\n",
    "A=torch.diag(1/(torch.tensor(range(matrix_size), dtype=torch.float32)+1))\n",
    "\n",
    "import scipy\n",
    "\n",
    "C = torch.tensor(scipy.stats.ortho_group.rvs(dim=matrix_size), dtype=torch.float32)\n",
    "A = torch.linalg.inv(C) @ A @ C\n",
    "\n",
    "f=nn.Linear(matrix_size,matrix_size, bias=False).cuda()\n",
    "f.weight=nn.Parameter(A.cuda(), requires_grad=False)\n",
    "#f.bias=nn.Parameter(torch.zeros_like(f.bias))\n",
    "\n",
    "h = nn.Linear(matrix_size, matrix_size, bias=False).cuda()\n",
    "h.weight=nn.Parameter(C.cuda(), requires_grad=False)\n",
    "\n",
    "result=np.zeros((matrix_size,))\n",
    "expr_1=t\n",
    "print(expr_1)\n",
    "t=[1]\n",
    "with torch.cuda.device(0):\n",
    "    for i in range(10000):\n",
    "        x = torch.empty((batch_size,matrix_size)).normal_(mean=0,std=1).cuda()\n",
    "        steps = torch.zeros((batch_size,1)).cuda()\n",
    "        t[0] = x/torch.linalg.norm(x, dim=-1, keepdim=True)\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim=-1, keepdim=True)[0]<0.9)\n",
    "        while torch.any((steps<2000) & cond):\n",
    "            g = modified(expr_1,f,batch_size)\n",
    "            s = grad_ascend_lr(g,t[0],0.9999,steps,2000)\n",
    "            t[0] = torch.where(cond, s[0], t[0])\n",
    "            steps = s[-1]\n",
    "            s = [1]\n",
    "            cond = (torch.max(torch.abs(h(t[0])), dim=-1, keepdim=True)[0]<0.9)\n",
    "        result+= torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=0).cpu().numpy()\n",
    "        print(i, list(np.where(result>0)[0]), end='\\r')\n",
    "        t = [1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "984b93d7-ce86-4681-8b50-226822b021c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6372e+04, 8.2040e+03, 4.5570e+03, 3.1630e+03, 1.9740e+03,\n",
       "       1.1010e+03, 5.7800e+02, 2.5800e+02, 1.4500e+02, 8.7000e+01,\n",
       "       6.5000e+01, 3.2000e+01, 2.7000e+01, 1.9000e+01, 8.0000e+00,\n",
       "       1.0000e+01, 4.0000e+00, 4.0000e+00, 2.0000e+00, 3.0000e+00,\n",
       "       3.0000e+00, 3.0000e+00, 3.0000e+00, 3.0000e+00, 1.0000e+00,\n",
       "       1.0000e+00, 1.0000e+00])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[np.where(result>0)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "221dc9d9-1440-4428-adaa-5f619324a2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(71181., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "############################################################3\n",
    "##############################################################\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e6694-30d8-41dd-a881-2d3ba6377d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102297/1415598044.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Q = complex_cbrt((Delta_1 + torch.sqrt(torch.tensor(Delta_1 ** 2 - 4 * Delta_0 ** 3, dtype=torch.complex64))) / 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sympy\n",
    "from sympy import Symbol, Mul, Pow, Add\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Prints the current allocated, reserved, and free memory on the GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "        free_memory = torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device)  # Free memory in reserved area\n",
    "        free_memory_mb = free_memory / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "        print(f\"Allocated memory: {allocated_memory:.2f} MB\")\n",
    "        print(f\"Reserved memory: {reserved_memory:.2f} MB\")\n",
    "        print(f\"Free memory: {free_memory_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "t=sympy.Symbol('t')\n",
    "sqrt_3=torch.sqrt(torch.tensor([3.])).cuda()\n",
    "\n",
    "\n",
    "def complex_cbrt(z):\n",
    "    \"\"\"Calculate the cubic root of a complex number.\"\"\"\n",
    "    # Calculate the magnitude and angle of the complex number\n",
    "    r = z.abs()  # Magnitude\n",
    "    theta = torch.angle(z)  # Angle in radians\n",
    "\n",
    "    # Calculate the cubic root of the magnitude and the angle divided by 3\n",
    "    root_magnitude = r ** (1/3)  # Magnitude of the cubic root\n",
    "    root_angle = theta / 3  # Angle of the cubic root\n",
    "\n",
    "    # Convert back to rectangular form (Cartesian coordinates)\n",
    "    root_real = root_magnitude * torch.cos(root_angle)\n",
    "    root_imag = root_magnitude * torch.sin(root_angle)\n",
    "\n",
    "    return root_real + 1j * root_imag  # Return as a complex number\n",
    "\n",
    "def quartic_solver(a,b,c,d,e):\n",
    "    e=e/a\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    Delta_0 = c ** 2 - 3 * b * d + 12 * e\n",
    "    Delta_1 = 2 * c ** 3 - 9 * b * c * d + 27 * b ** 2 * e + 27 * d ** 2 - 72 * c * e\n",
    "\n",
    "    Q = complex_cbrt((Delta_1 + torch.sqrt(torch.tensor(Delta_1 ** 2 - 4 * Delta_0 ** 3, dtype=torch.complex64))) / 2)\n",
    "    \n",
    "    # Calculate p and q\n",
    "    p = c - 0.375 * b ** 2\n",
    "    q = (0.5*b) ** 3 - 0.5 * b * c + d    \n",
    "    S = torch.sqrt(-2 / 3 * p + (Q + Delta_0 / Q) / 3) / 2\n",
    "    S_1 = 0.5*torch.sqrt(-4 * S ** 2 - 2 * p + q / S)\n",
    "    S_2 = 0.5*torch.sqrt(-4 * S ** 2 - 2 * p - q / S)\n",
    "    b=-0.25*b\n",
    "\n",
    "    lr_1 = torch.real(b - S + S_1)\n",
    "    lr_2 = torch.real(b - S - S_1)\n",
    "    lr_3 = torch.real(b + S + S_2)\n",
    "    lr_4 = torch.real(b + S - S_2)\n",
    "    \n",
    "    return torch.cat([lr_1,lr_2,lr_3,lr_4],dim=-1)\n",
    "\n",
    "def cubic_solver(a,b,c,d):\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    p = c - b ** 2 / 3\n",
    "    q = 2/27*b**3 - b*c/3 + d\n",
    "    Q = (p/3)**3 + (q/2)**2\n",
    "    S = torch.sqrt(Q)\n",
    "    alpha = -q/2 + S\n",
    "    beta = -q/2 - S\n",
    "    alpha = complex_cbrt(alpha)\n",
    "    beta = complex_cbrt(beta)\n",
    "    \n",
    "    lr1 = torch.real(alpha+beta)\n",
    "    lr2 = torch.real((-(alpha+beta)+1j*sqrt_3*(alpha-beta))/2)\n",
    "    lr3 = torch.real((-(alpha+beta)-1j*sqrt_3*(alpha-beta))/2)\n",
    "    return torch.cat([lr1,lr2,lr3, torch.zeros_like(lr1)],dim=-1)\n",
    "\n",
    "\n",
    "def quadratic_solver(c,d,e):\n",
    "    D = torch.sqrt(d ** 2 - 4 * e * c)\n",
    "    x1 = torch.real((-d + D)/(2*c))\n",
    "    x2 = torch.real((-d - D)/(2*c))\n",
    "    return torch.cat([x1,x2,torch.zeros_like(x1),torch.zeros_like(x1)], dim=-1)\n",
    "\n",
    "def polynomial_solver(a,b,c,d,e):\n",
    "    return torch.where(a!=0, \n",
    "                       quartic_solver(a,b,c,d,e), torch.where(\n",
    "                        b!=0,\n",
    "                        cubic_solver(b,c,d,e), torch.where(\n",
    "                        c!=0,\n",
    "                        quadratic_solver(c,d,e), torch.where(\n",
    "                        d!=0, \n",
    "                        (e/d).repeat((len(a.shape)-1)*(1,)+(4,)), \n",
    "                        e.repeat((len(a.shape)-1)*(1,)+(4,))))))\n",
    "\n",
    "\n",
    "def optimal_lr(A, x):\n",
    "    \"\"\"Calculate optimal learning rate based on matrix A and vector x.\"\"\"\n",
    "    # Compute intermediate vectors\n",
    "    u = A(x)\n",
    "    v = A(u)\n",
    "    w = A(v)\n",
    "    \n",
    "    # Compute inner products\n",
    "    a_0 = torch.einsum('ij,ij->i', x, x).unsqueeze(-1)\n",
    "    a_1 = torch.einsum('ij,ij->i', u, x).unsqueeze(-1)\n",
    "    a_2 = torch.einsum('ij,ij->i', v, x).unsqueeze(-1)\n",
    "    a_3 = torch.einsum('ij,ij->i', w, x).unsqueeze(-1)\n",
    "    a_4 = torch.einsum('ij,ij->i', w, u).unsqueeze(-1)\n",
    "    a_5 = torch.einsum('ij,ij->i', w, v).unsqueeze(-1)\n",
    "    a_6 = torch.einsum('ij,ij->i', w, w).unsqueeze(-1)\n",
    "\n",
    "    # Calculate r_0, r_1, r_2\n",
    "    r_0 = 4 * a_2 / (a_1 * a_1) - 2 * a_1 / (a_1 * a_0) - 2 * a_3 / (a_1 * a_2) - 2 * a_3 / (a_1 * a_2) + a_4 / (a_2 * a_2) + a_2 / (a_0 * a_2) - 2 * a_1 / (a_1 * a_0) + a_0 / (a_0 * a_0) + a_2 / (a_0 * a_2)\n",
    "    r_1 = 4 * a_3 / (a_1 * a_1) - 2 * a_2 / (a_1 * a_0) - 2 * a_4 / (a_1 * a_2) - 2 * a_4 / (a_1 * a_2) + a_5 / (a_2 * a_2) + a_3 / (a_0 * a_2) - 2 * a_2 / (a_1 * a_0) + a_1 / (a_0 * a_0) + a_3 / (a_0 * a_2)\n",
    "    r_2 = 4 * a_4 / (a_1 * a_1) - 2 * a_3 / (a_1 * a_0) - 2 * a_5 / (a_1 * a_2) - 2 * a_5 / (a_1 * a_2) + a_6 / (a_2 * a_2) + a_4 / (a_0 * a_2) - 2 * a_3 / (a_1 * a_0) + a_2 / (a_0 * a_0) + a_4 / (a_0 * a_2)\n",
    "\n",
    "    # Calculate q and p\n",
    "    q_1 = 2 * a_2 / a_1 - a_1 / a_0 - a_3 / a_2\n",
    "    q_2 = 2 * a_3 / a_1 - a_2 / a_0 - a_4 / a_2\n",
    "    p_0 = a_0\n",
    "    p_1 = a_1\n",
    "    p_2 = a_2\n",
    "\n",
    "    a = r_0 * r_1 * q_2 - 2 * r_0 * q_1 * r_2\n",
    "    b = p_0 * r_1 * r_2 - 2 * p_1 * r_0 * r_2 + p_2 * r_0 * r_1 - 2 * q_1 * q_2 * r_0\n",
    "    c = 3 * p_0 * r_1 * q_2 - 3 * r_0 * p_1 * q_2\n",
    "    d = 2 * p_0 * q_1 * q_2 + 2 * p_0 * r_1 * p_2 - p_0 * p_1 * r_2 - r_0 * p_1 * p_2\n",
    "    e = 2 * p_0 * q_1 * p_2 - p_0 * p_1 * q_2\n",
    "    lr = polynomial_solver(a,b,c,d,e)\n",
    "    g = a_1 + 2 * lr * q_1 + lr ** 2 * r_1\n",
    "    f = a_0 + lr ** 2 * r_0\n",
    "    h = a_2 + 2 * lr * q_2 + lr ** 2 * r_2\n",
    "    eigenness = g**2/(f*h)\n",
    "    n = torch.argmax(eigenness, dim=-1, keepdim=True)\n",
    "    lr = torch.gather(lr, -1, n)\n",
    "    return torch.gather(eigenness, -1, n), (-lr / a_0) * x + (2 * lr / a_1) * u + (-lr / a_2) * v, n, lr\n",
    "\n",
    "\n",
    "def update_vector(x, v):\n",
    "    x=x+v\n",
    "    return x/torch.linalg.norm(x, dim=-1).unsqueeze(-1)\n",
    "\n",
    "def grad_ascend_lr(A,x,threshold,steps_already,steps_max):\n",
    "    e,v,i,l = optimal_lr(A,x)\n",
    "    x = update_vector(x, v)\n",
    "    steps = steps_already+1\n",
    "    cond = (e<threshold) & (steps<steps_max)\n",
    "    while cond.any():\n",
    "        x = torch.where(cond, update_vector(x, v), x)\n",
    "        steps = torch.where(cond, steps+1, steps)\n",
    "        e,v,i,l=optimal_lr(A,x)\n",
    "        cond=(e<threshold)&(steps<steps_max)\n",
    "    f=A(x)\n",
    "    return x, torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1).unsqueeze(-1)**2, (f-(torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1)).unsqueeze(-1)**2*x)/torch.linalg.norm(f, dim=-1).unsqueeze(-1),steps\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "class modified(nn.Module):\n",
    "    def __init__(self,expr,A,batch_size):\n",
    "        super(modified, self).__init__()\n",
    "        self.parts=[]\n",
    "        for arg in expr.args:\n",
    "            self.parts.append(modified(arg,A,batch_size))\n",
    "        if expr.is_Number:\n",
    "            self.param = nn.Parameter(torch.random.uniform((batch_size,1))*float(expr))\n",
    "        self.f=torch.rand(batch_size,1).cuda()\n",
    "        self.t=(torch.rand(batch_size,1).cuda()*torch.max(torch.cat([self.f**2,(1-self.f)**2],dim=1),dim=1)[0].unsqueeze(-1)).cuda()\n",
    "        self.z=torch.max(torch.cat([self.t**2,(self.t-self.f**2)**2,(self.t-(1-self.f)**2)**2], dim=1), dim=1)[0].unsqueeze(-1)\n",
    "        if expr == sympy.Symbol('t'):\n",
    "            self.A=A\n",
    "        self.expr=expr\n",
    "    def forward_1(self,x):\n",
    "            y=self.A(x)-self.f*x\n",
    "            y=self.A(y)-self.f*y\n",
    "            y=self.t*x-y\n",
    "            z=self.A(y)-self.f*y\n",
    "            z=self.A(z)-self.f*z\n",
    "            y=self.t*y-z\n",
    "            return self.z*x-y\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.forward_1(x)\n",
    "        y = self.forward_1(y)\n",
    "        y = self.forward_1(y)\n",
    "        return self.forward_1(y) \n",
    "        if self.expr == sympy.Symbol('t'):\n",
    "            return self.A(x)\n",
    "\n",
    "        if self.expr.is_Number:\n",
    "            return self.param*x\n",
    "\n",
    "        if self.expr.is_Add:\n",
    "            # Initialize res as float zeros to avoid dtype conflicts\n",
    "            res = torch.zeros_like(x)\n",
    "            for arg in self.parts:\n",
    "                res += arg(x)\n",
    "            return res\n",
    "\n",
    "        if self.expr.is_Mul:\n",
    "            # product: to mimic polynomial multiplication\n",
    "            for arg in self.parts:\n",
    "                x = arg(x)\n",
    "            return x\n",
    "\n",
    "        if self.expr.is_Pow:\n",
    "            for _ in range(self.parts[1]):\n",
    "                x = self.parts[0](x)\n",
    "            return x\n",
    " \n",
    "\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "batch_size=10000\n",
    "matrix_size=1000\n",
    "A=torch.diag(1/(torch.tensor(range(matrix_size), dtype=torch.float32)+1))\n",
    "\n",
    "import scipy\n",
    "\n",
    "C = torch.tensor(scipy.stats.ortho_group.rvs(dim=matrix_size), dtype=torch.float32)\n",
    "A = torch.linalg.inv(C) @ A @ C\n",
    "\n",
    "f=nn.Linear(matrix_size,matrix_size, bias=False).cuda()\n",
    "f.weight=nn.Parameter(A.cuda(), requires_grad=False)\n",
    "#f.bias=nn.Parameter(torch.zeros_like(f.bias))\n",
    "\n",
    "h = nn.Linear(matrix_size, matrix_size, bias=False).cuda()\n",
    "h.weight=nn.Parameter(C.cuda(), requires_grad=False)\n",
    "\n",
    "result=np.zeros((matrix_size,))\n",
    "expr_1=t\n",
    "print(expr_1)\n",
    "t=[1]\n",
    "with torch.cuda.device(0):\n",
    "    for i in range(10000):\n",
    "        x = torch.empty((batch_size,matrix_size)).normal_(mean=0,std=1).cuda()\n",
    "        steps = torch.zeros((batch_size,1)).cuda()\n",
    "        t[0] = x/torch.linalg.norm(x, dim=-1, keepdim=True)\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim=-1, keepdim=True)[0]<0.9)\n",
    "        while torch.any((steps<2000) & cond):\n",
    "            g = modified(expr_1,f,batch_size)\n",
    "            s = grad_ascend_lr(g,t[0],0.9999,steps,2000)\n",
    "            t[0] = torch.where(cond, s[0], t[0])\n",
    "            steps = s[-1]\n",
    "            s = [1]\n",
    "            cond = (torch.max(torch.abs(h(t[0])), dim=-1, keepdim=True)[0]<0.9)\n",
    "        result+= torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=0).cpu().numpy()\n",
    "        print(i, list(np.where(result>0)[0]), end='\\r')\n",
    "        t = [1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d6bf3-7d44-4d58-b894-a7005f2c815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(steps), torch.max(steps), torch.sum(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ed6311-712a-427a-a252-e8f73a770489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 16, 17,\n",
      "       21, 22]),)\n"
     ]
    }
   ],
   "source": [
    "print(np.where(result>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be537f64-0050-4315-9ce2-a11c38aae97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_ascend_lr_transform(C,x,j,steps_already):\n",
    "    f=jra.uniform(key = jra.PRNGKey(2*j))\n",
    "    B=C-f*I\n",
    "    B=B@B\n",
    "    t=jra.uniform(key = jra.PRNGKey(2*j+1))*f**2\n",
    "    B=t*I-B\n",
    "    B=B@B\n",
    "    z=max(t**2, (t-f**2)**2, (t-(1-f)**2)**2)\n",
    "    B=z*I-B\n",
    "    \n",
    "    return grad_ascend_lr(B,x, steps_already)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceacaa97-e352-4110-ba6f-3a9069466313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ce7cd38-7787-409c-9f8b-c4cf6557a00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n"
     ]
    }
   ],
   "source": [
    "x=torch.empty((20,10)).normal_(mean=0,std=1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b9d49-3c8b-42ad-90a7-00d40f5fdfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "#######################################################################################################\n",
    "###########################################################################################################\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a437303-d740-43a5-8986-ad6eba13e5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6388/1986055406.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Q = complex_cbrt((Delta_1 + torch.sqrt(torch.tensor(Delta_1 ** 2 - 4 * Delta_0 ** 3, dtype=torch.complex64))) / 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [0, 1, 2, 3, 12, 16, 18, 31, 32, 39, 60, 77, 98, 99, 113, 114, 115, 118, 120, 124, 155, 160, 163, 235, 237, 701, 787, 813, 839, 858, 873, 890, 896, 907, 935, 945, 948, 970, 987, 997, 1004, 1018, 1019, 1020, 1021, 1022, 1023]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 254\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many((steps\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m2000\u001b[39m) \u001b[38;5;241m&\u001b[39m cond):\n\u001b[1;32m    253\u001b[0m     g \u001b[38;5;241m=\u001b[39m modified(expr_1,f,batch_size)\n\u001b[0;32m--> 254\u001b[0m     s \u001b[38;5;241m=\u001b[39m grad_ascend_lr(g,t[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m0.9999\u001b[39m,steps,\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m    255\u001b[0m     t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(cond, s[\u001b[38;5;241m0\u001b[39m], t[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    256\u001b[0m     steps \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 154\u001b[0m, in \u001b[0;36mgrad_ascend_lr\u001b[0;34m(A, x, threshold, steps_already, steps_max)\u001b[0m\n\u001b[1;32m    152\u001b[0m steps \u001b[38;5;241m=\u001b[39m steps_already\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    153\u001b[0m cond \u001b[38;5;241m=\u001b[39m (e\u001b[38;5;241m<\u001b[39mthreshold) \u001b[38;5;241m&\u001b[39m (steps\u001b[38;5;241m<\u001b[39msteps_max)\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    155\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(cond, update_vector(x, v), x)\n\u001b[1;32m    156\u001b[0m     steps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(cond, steps\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, steps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sympy\n",
    "from sympy import Symbol, Mul, Pow, Add\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Prints the current allocated, reserved, and free memory on the GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        allocated_memory = torch.cuda.memory_allocated(device) / (1024 ** 2)  # Convert to MB\n",
    "        reserved_memory = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "        free_memory = torch.cuda.get_device_properties(device).total_memory - torch.cuda.memory_allocated(device)  # Free memory in reserved area\n",
    "        free_memory_mb = free_memory / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "        print(f\"Allocated memory: {allocated_memory:.2f} MB\")\n",
    "        print(f\"Reserved memory: {reserved_memory:.2f} MB\")\n",
    "        print(f\"Free memory: {free_memory_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "t=sympy.Symbol('t')\n",
    "sqrt_3=torch.sqrt(torch.tensor([3.])).cuda()\n",
    "\n",
    "\n",
    "def complex_cbrt(z):\n",
    "    \"\"\"Calculate the cubic root of a complex number.\"\"\"\n",
    "    # Calculate the magnitude and angle of the complex number\n",
    "    r = z.abs()  # Magnitude\n",
    "    theta = torch.angle(z)  # Angle in radians\n",
    "\n",
    "    # Calculate the cubic root of the magnitude and the angle divided by 3\n",
    "    root_magnitude = r ** (1/3)  # Magnitude of the cubic root\n",
    "    root_angle = theta / 3  # Angle of the cubic root\n",
    "\n",
    "    # Convert back to rectangular form (Cartesian coordinates)\n",
    "    root_real = root_magnitude * torch.cos(root_angle)\n",
    "    root_imag = root_magnitude * torch.sin(root_angle)\n",
    "\n",
    "    return root_real + 1j * root_imag  # Return as a complex number\n",
    "\n",
    "def quartic_solver(a,b,c,d,e):\n",
    "    e=e/a\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    Delta_0 = c ** 2 - 3 * b * d + 12 * e\n",
    "    Delta_1 = 2 * c ** 3 - 9 * b * c * d + 27 * b ** 2 * e + 27 * d ** 2 - 72 * c * e\n",
    "\n",
    "    Q = complex_cbrt((Delta_1 + torch.sqrt(torch.tensor(Delta_1 ** 2 - 4 * Delta_0 ** 3, dtype=torch.complex64))) / 2)\n",
    "    \n",
    "    # Calculate p and q\n",
    "    p = c - 0.375 * b ** 2\n",
    "    q = (0.5*b) ** 3 - 0.5 * b * c + d    \n",
    "    S = torch.sqrt(-2 / 3 * p + (Q + Delta_0 / Q) / 3) / 2\n",
    "    S_1 = 0.5*torch.sqrt(-4 * S ** 2 - 2 * p + q / S)\n",
    "    S_2 = 0.5*torch.sqrt(-4 * S ** 2 - 2 * p - q / S)\n",
    "    b=-0.25*b\n",
    "\n",
    "    lr_1 = torch.real(b - S + S_1)\n",
    "    lr_2 = torch.real(b - S - S_1)\n",
    "    lr_3 = torch.real(b + S + S_2)\n",
    "    lr_4 = torch.real(b + S - S_2)\n",
    "    \n",
    "    return torch.cat([lr_1,lr_2,lr_3,lr_4],dim=-1)\n",
    "\n",
    "def cubic_solver(a,b,c,d):\n",
    "    d=d/a\n",
    "    c=c/a\n",
    "    b=b/a\n",
    "    p = c - b ** 2 / 3\n",
    "    q = 2/27*b**3 - b*c/3 + d\n",
    "    Q = (p/3)**3 + (q/2)**2\n",
    "    S = torch.sqrt(Q)\n",
    "    alpha = -q/2 + S\n",
    "    beta = -q/2 - S\n",
    "    alpha = complex_cbrt(alpha)\n",
    "    beta = complex_cbrt(beta)\n",
    "    \n",
    "    lr1 = torch.real(alpha+beta)\n",
    "    lr2 = torch.real((-(alpha+beta)+1j*sqrt_3*(alpha-beta))/2)\n",
    "    lr3 = torch.real((-(alpha+beta)-1j*sqrt_3*(alpha-beta))/2)\n",
    "    return torch.cat([lr1,lr2,lr3, torch.zeros_like(lr1)],dim=-1)\n",
    "\n",
    "\n",
    "def quadratic_solver(c,d,e):\n",
    "    D = torch.sqrt(d ** 2 - 4 * e * c)\n",
    "    x1 = torch.real((-d + D)/(2*c))\n",
    "    x2 = torch.real((-d - D)/(2*c))\n",
    "    return torch.cat([x1,x2,torch.zeros_like(x1),torch.zeros_like(x1)], dim=-1)\n",
    "\n",
    "def polynomial_solver(a,b,c,d,e):\n",
    "    return torch.where(a!=0, \n",
    "                       quartic_solver(a,b,c,d,e), torch.where(\n",
    "                        b!=0,\n",
    "                        cubic_solver(b,c,d,e), torch.where(\n",
    "                        c!=0,\n",
    "                        quadratic_solver(c,d,e), torch.where(\n",
    "                        d!=0, \n",
    "                        (e/d).repeat((len(a.shape)-1)*(1,)+(4,)), \n",
    "                        e.repeat((len(a.shape)-1)*(1,)+(4,))))))\n",
    "\n",
    "\n",
    "def optimal_lr(A, x):\n",
    "    \"\"\"Calculate optimal learning rate based on matrix A and vector x.\"\"\"\n",
    "    # Compute intermediate vectors\n",
    "    u = A(x)\n",
    "    v = A(u)\n",
    "    w = A(v)\n",
    "    \n",
    "    # Compute inner products\n",
    "    a_0 = torch.einsum('ij,ij->i', x, x).unsqueeze(-1)\n",
    "    a_1 = torch.einsum('ij,ij->i', u, x).unsqueeze(-1)\n",
    "    a_2 = torch.einsum('ij,ij->i', v, x).unsqueeze(-1)\n",
    "    a_3 = torch.einsum('ij,ij->i', w, x).unsqueeze(-1)\n",
    "    a_4 = torch.einsum('ij,ij->i', w, u).unsqueeze(-1)\n",
    "    a_5 = torch.einsum('ij,ij->i', w, v).unsqueeze(-1)\n",
    "    a_6 = torch.einsum('ij,ij->i', w, w).unsqueeze(-1)\n",
    "\n",
    "    # Calculate r_0, r_1, r_2\n",
    "    r_0 = 4 * a_2 / (a_1 * a_1) - 2 * a_1 / (a_1 * a_0) - 2 * a_3 / (a_1 * a_2) - 2 * a_3 / (a_1 * a_2) + a_4 / (a_2 * a_2) + a_2 / (a_0 * a_2) - 2 * a_1 / (a_1 * a_0) + a_0 / (a_0 * a_0) + a_2 / (a_0 * a_2)\n",
    "    r_1 = 4 * a_3 / (a_1 * a_1) - 2 * a_2 / (a_1 * a_0) - 2 * a_4 / (a_1 * a_2) - 2 * a_4 / (a_1 * a_2) + a_5 / (a_2 * a_2) + a_3 / (a_0 * a_2) - 2 * a_2 / (a_1 * a_0) + a_1 / (a_0 * a_0) + a_3 / (a_0 * a_2)\n",
    "    r_2 = 4 * a_4 / (a_1 * a_1) - 2 * a_3 / (a_1 * a_0) - 2 * a_5 / (a_1 * a_2) - 2 * a_5 / (a_1 * a_2) + a_6 / (a_2 * a_2) + a_4 / (a_0 * a_2) - 2 * a_3 / (a_1 * a_0) + a_2 / (a_0 * a_0) + a_4 / (a_0 * a_2)\n",
    "\n",
    "    # Calculate q and p\n",
    "    q_1 = 2 * a_2 / a_1 - a_1 / a_0 - a_3 / a_2\n",
    "    q_2 = 2 * a_3 / a_1 - a_2 / a_0 - a_4 / a_2\n",
    "    p_0 = a_0\n",
    "    p_1 = a_1\n",
    "    p_2 = a_2\n",
    "\n",
    "    a = r_0 * r_1 * q_2 - 2 * r_0 * q_1 * r_2\n",
    "    b = p_0 * r_1 * r_2 - 2 * p_1 * r_0 * r_2 + p_2 * r_0 * r_1 - 2 * q_1 * q_2 * r_0\n",
    "    c = 3 * p_0 * r_1 * q_2 - 3 * r_0 * p_1 * q_2\n",
    "    d = 2 * p_0 * q_1 * q_2 + 2 * p_0 * r_1 * p_2 - p_0 * p_1 * r_2 - r_0 * p_1 * p_2\n",
    "    e = 2 * p_0 * q_1 * p_2 - p_0 * p_1 * q_2\n",
    "    lr = polynomial_solver(a,b,c,d,e)\n",
    "    g = a_1 + 2 * lr * q_1 + lr ** 2 * r_1\n",
    "    f = a_0 + lr ** 2 * r_0\n",
    "    h = a_2 + 2 * lr * q_2 + lr ** 2 * r_2\n",
    "    eigenness = g**2/(f*h)\n",
    "    n = torch.argmax(eigenness, dim=-1, keepdim=True)\n",
    "    lr = torch.gather(lr, -1, n)\n",
    "    return torch.gather(eigenness, -1, n), (-lr / a_0) * x + (2 * lr / a_1) * u + (-lr / a_2) * v, n, lr\n",
    "\n",
    "\n",
    "def update_vector(x, v):\n",
    "    x=x+v\n",
    "    return x/torch.linalg.norm(x, dim=-1).unsqueeze(-1)\n",
    "\n",
    "def grad_ascend_lr(A,x,threshold,steps_already,steps_max):\n",
    "    e,v,i,l = optimal_lr(A,x)\n",
    "    x = update_vector(x, v)\n",
    "    steps = steps_already+1\n",
    "    cond = (e<threshold) & (steps<steps_max)\n",
    "    while cond.any():\n",
    "        x = torch.where(cond, update_vector(x, v), x)\n",
    "        steps = torch.where(cond, steps+1, steps)\n",
    "        e,v,i,l=optimal_lr(A,x)\n",
    "        cond=(e<threshold)&(steps<steps_max)\n",
    "    f=A(x)\n",
    "    return x, torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1).unsqueeze(-1)**2, (f-(torch.einsum('ij,ij->i',f,x)/torch.linalg.norm(x,dim=-1)).unsqueeze(-1)**2*x)/torch.linalg.norm(f, dim=-1).unsqueeze(-1),steps\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def add(x,y):\n",
    "    return x+y\n",
    "\n",
    "class modified(nn.Module):\n",
    "    def __init__(self,expr,A,batch_size):\n",
    "        super(modified, self).__init__()\n",
    "        self.parts=[]\n",
    "        for arg in expr.args:\n",
    "            self.parts.append(modified(arg,A,batch_size))\n",
    "        if expr.is_Number:\n",
    "            self.param = nn.Parameter(torch.random.uniform((batch_size,1))*float(expr))\n",
    "        self.f=torch.rand(batch_size,1).cuda()\n",
    "        self.t=(torch.rand(batch_size,1).cuda()*torch.max(torch.cat([self.f**2,(1-self.f)**2],dim=1),dim=1)[0].unsqueeze(-1)).cuda()\n",
    "        self.z=torch.max(torch.cat([self.t**2,(self.t-self.f**2)**2,(self.t-(1-self.f)**2)**2], dim=1), dim=1)[0].unsqueeze(-1)\n",
    "        if expr == sympy.Symbol('t'):\n",
    "            self.A=A\n",
    "        self.expr=expr\n",
    "    def forward_1(self,x):\n",
    "            y=self.A(x)-self.f*x\n",
    "            y=self.A(y)-self.f*y\n",
    "            y=self.t*x-y\n",
    "            z=self.A(y)-self.f*y\n",
    "            z=self.A(z)-self.f*z\n",
    "            y=self.t*y-z\n",
    "            return self.z*x-y\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.forward_1(x)\n",
    "        y = self.forward_1(y)\n",
    "        y = self.forward_1(y)\n",
    "        return self.forward_1(y) \n",
    "        if self.expr == sympy.Symbol('t'):\n",
    "            return self.A(x)\n",
    "\n",
    "        if self.expr.is_Number:\n",
    "            return self.param*x\n",
    "\n",
    "        if self.expr.is_Add:\n",
    "            # Initialize res as float zeros to avoid dtype conflicts\n",
    "            res = torch.zeros_like(x)\n",
    "            for arg in self.parts:\n",
    "                res += arg(x)\n",
    "            return res\n",
    "\n",
    "        if self.expr.is_Mul:\n",
    "            # product: to mimic polynomial multiplication\n",
    "            for arg in self.parts:\n",
    "                x = arg(x)\n",
    "            return x\n",
    "\n",
    "        if self.expr.is_Pow:\n",
    "            for _ in range(self.parts[1]):\n",
    "                x = self.parts[0](x)\n",
    "            return x\n",
    " \n",
    "\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "batch_size=16384\n",
    "matrix_size=1024\n",
    "A=torch.diag(torch.tensor(range(matrix_size), dtype=torch.float32)/matrix_size)\n",
    "\n",
    "import scipy\n",
    "\n",
    "C = torch.tensor(scipy.stats.ortho_group.rvs(dim=matrix_size), dtype=torch.float32)\n",
    "A = torch.linalg.inv(C) @ A @ C\n",
    "\n",
    "f=nn.Linear(matrix_size,matrix_size, bias=False).cuda()\n",
    "f.weight=nn.Parameter(A.cuda(), requires_grad=False)\n",
    "#f.bias=nn.Parameter(torch.zeros_like(f.bias))\n",
    "\n",
    "h = nn.Linear(matrix_size, matrix_size, bias=False).cuda()\n",
    "h.weight=nn.Parameter(C.cuda(), requires_grad=False)\n",
    "\n",
    "result=np.zeros((matrix_size,))\n",
    "expr_1=t\n",
    "print(expr_1)\n",
    "t=[1]\n",
    "with torch.cuda.device(0):\n",
    "    for i in range(10000):\n",
    "        x = torch.empty((batch_size,matrix_size)).normal_(mean=0,std=1).cuda()\n",
    "        steps = torch.zeros((batch_size,1)).cuda()\n",
    "        t[0] = x/torch.linalg.norm(x, dim=-1, keepdim=True)\n",
    "        cond = (torch.max(torch.abs(h(t[0])), dim=-1, keepdim=True)[0]<0.9)\n",
    "        while torch.any((steps<2000) & cond):\n",
    "            g = modified(expr_1,f,batch_size)\n",
    "            s = grad_ascend_lr(g,t[0],0.9999,steps,2000)\n",
    "            t[0] = torch.where(cond, s[0], t[0])\n",
    "            steps = s[-1]\n",
    "            s = [1]\n",
    "            cond = (torch.max(torch.abs(h(t[0])), dim=-1, keepdim=True)[0]<0.9)\n",
    "        result+= torch.sum(torch.where(torch.abs(h(t[0]))>=0.9,1,0),dim=0).cpu().numpy()\n",
    "        print(i, list(np.where(result>0)[0]), end='\\r')\n",
    "        t = [1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9151a758-af30-4155-87ee-52f2d820bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47. 13.  5.  2.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  3.  5.  9. 48.]\n"
     ]
    }
   ],
   "source": [
    "print(result[np.where(result>0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44f2a7cd-10e8-4b8a-ba0a-1c854ac38a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1185432., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7854a3a-6ead-4916-9503-fe46bdf63bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
